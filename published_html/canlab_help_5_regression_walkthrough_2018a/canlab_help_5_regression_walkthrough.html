
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>canlab_help_5_regression_walkthrough</title><meta name="generator" content="MATLAB 9.4"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2018-07-30"><meta name="DC.source" content="canlab_help_5_regression_walkthrough.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h2>Contents</h2><div><ul><li><a href="#1">Load sample data</a></li><li><a href="#2">Visualize the mask</a></li><li><a href="#3">Visualize summary of brain coverage</a></li><li><a href="#4">BLOCK 4</a></li><li><a href="#5">BLOCK 5</a></li><li><a href="#6">BLOCK 6</a></li><li><a href="#7">BLOCK 7</a></li><li><a href="#8">BLOCK 8: Look for signal in ventricles, white matter, outside of brain</a></li><li><a href="#9">BLOCK 9: Compare results to meta-analysis for positive controls</a></li><li><a href="#10">BLOCK 10: Apply gray-matter mask and show FDR-thresholded results</a></li><li><a href="#11">BLOCK 11: Refine analysis by removing outlier and ranking predictor values</a></li><li><a href="#12">Block 12: Extract and plot data from (biased) regions of interest</a></li><li><a href="#13">Block 13: Extract and plot data from (biased) regions of interest</a></li><li><a href="#14">Block 14: Extract and plot data from unbiased regions of interest</a></li><li><a href="#15">Block 15: Multivariate prediction from unbiased ROI averages</a></li></ul></div><h2 id="1">Load sample data</h2><p>BLOCK 1 ---------------------------------------------------------------</p><pre class="codeinput"><span class="comment">% Load sample data using load_image_set(), which produces an fmri_data</span>
<span class="comment">% object. Data loading exceeds the scope of this tutorial, but a more</span>
<span class="comment">% indepth demosntration may be provided by canlab_help_2_load_a_sample_dataset.m</span>

<span class="comment">% These are [Reappraise - Look Neg] contrast images, one image per person</span>

[image_obj, networknames, imagenames] = load_image_set(<span class="string">'emotionreg'</span>);

<span class="comment">% Summarize and print a table with characteristics of the data:</span>
desc = descriptives(image_obj);

<span class="comment">% Load behavioral data</span>
<span class="comment">% This is "Reappraisal success", one score per person, in our example</span>
<span class="comment">% If you do not have the file on your path, you will get an error.</span>

beh = importdata(<span class="string">'Wager_2008_emotionreg_behavioral_data.txt'</span>)
success = beh.data(:, 2);           <span class="comment">% Reappraisal success</span>

<span class="comment">% Load a mask that we would like to apply to analysis/results</span>

mask = which(<span class="string">'gray_matter_mask.img'</span>)

maskdat = fmri_data(mask, <span class="string">'noverbose'</span>);
</pre><pre class="codeoutput">Loaded images:
/Users/torwager/Documents/GitHub/CanlabCore/CanlabCore/Sample_datasets/Wager_et_al_2008_Neuron_EmotionReg/Wager_2008_emo_reg_vs_look_neg_contrast_images.nii
/Users/torwager/Documents/GitHub/CanlabCore/CanlabCore/Sample_datasets/Wager_et_al_2008_Neuron_EmotionReg/Wager_2008_emo_reg_vs_look_neg_contrast_images.nii
/Users/torwager/Documents/GitHub/CanlabCore/CanlabCore/Sample_datasets/Wager_et_al_2008_Neuron_EmotionReg/Wager_2008_emo_reg_vs_look_neg_contrast_images.nii
/Users/torwager/Documents/GitHub/CanlabCore/CanlabCore/Sample_datasets/Wager_et_al_2008_Neuron_EmotionReg/Wager_2008_emo_reg_vs_look_neg_contrast_images.nii
/Users/torwager/Documents/GitHub/CanlabCore/CanlabCore/Sample_datasets/Wager_et_al_2008_Neuron_EmotionReg/Wager_2008_emo_reg_vs_look_neg_contrast_images.nii
/Users/torwager/Documents/GitHub/CanlabCore/CanlabCore/Sample_datasets/Wager_et_al_2008_Neuron_EmotionReg/Wager_2008_emo_reg_vs_look_neg_contrast_images.nii
/Users/torwager/Documents/GitHub/CanlabCore/CanlabCore/Sample_datasets/Wager_et_al_2008_Neuron_EmotionReg/Wager_2008_emo_reg_vs_look_neg_contrast_images.nii
/Users/torwager/Documents/GitHub/CanlabCore/CanlabCore/Sample_datasets/Wager_et_al_2008_Neuron_EmotionReg/Wager_2008_emo_reg_vs_look_neg_contrast_images.nii
/Users/torwager/Documents/GitHub/CanlabCore/CanlabCore/Sample_datasets/Wager_et_al_2008_Neuron_EmotionReg/Wager_2008_emo_reg_vs_look_neg_contrast_images.nii
/Users/torwager/Documents/GitHub/CanlabCore/CanlabCore/Sample_datasets/Wager_et_al_2008_Neuron_EmotionReg/Wager_2008_emo_reg_vs_look_neg_contrast_images.nii
/Users/torwager/Documents/GitHub/CanlabCore/CanlabCore/Sample_datasets/Wager_et_al_2008_Neuron_EmotionReg/Wager_2008_emo_reg_vs_look_neg_contrast_images.nii
/Users/torwager/Documents/GitHub/CanlabCore/CanlabCore/Sample_datasets/Wager_et_al_2008_Neuron_EmotionReg/Wager_2008_emo_reg_vs_look_neg_contrast_images.nii
/Users/torwager/Documents/GitHub/CanlabCore/CanlabCore/Sample_datasets/Wager_et_al_2008_Neuron_EmotionReg/Wager_2008_emo_reg_vs_look_neg_contrast_images.nii
/Users/torwager/Documents/GitHub/CanlabCore/CanlabCore/Sample_datasets/Wager_et_al_2008_Neuron_EmotionReg/Wager_2008_emo_reg_vs_look_neg_contrast_images.nii
/Users/torwager/Documents/GitHub/CanlabCore/CanlabCore/Sample_datasets/Wager_et_al_2008_Neuron_EmotionReg/Wager_2008_emo_reg_vs_look_neg_contrast_images.nii
/Users/torwager/Documents/GitHub/CanlabCore/CanlabCore/Sample_datasets/Wager_et_al_2008_Neuron_EmotionReg/Wager_2008_emo_reg_vs_look_neg_contrast_images.nii
/Users/torwager/Documents/GitHub/CanlabCore/CanlabCore/Sample_datasets/Wager_et_al_2008_Neuron_EmotionReg/Wager_2008_emo_reg_vs_look_neg_contrast_images.nii
/Users/torwager/Documents/GitHub/CanlabCore/CanlabCore/Sample_datasets/Wager_et_al_2008_Neuron_EmotionReg/Wager_2008_emo_reg_vs_look_neg_contrast_images.nii
/Users/torwager/Documents/GitHub/CanlabCore/CanlabCore/Sample_datasets/Wager_et_al_2008_Neuron_EmotionReg/Wager_2008_emo_reg_vs_look_neg_contrast_images.nii
/Users/torwager/Documents/GitHub/CanlabCore/CanlabCore/Sample_datasets/Wager_et_al_2008_Neuron_EmotionReg/Wager_2008_emo_reg_vs_look_neg_contrast_images.nii
/Users/torwager/Documents/GitHub/CanlabCore/CanlabCore/Sample_datasets/Wager_et_al_2008_Neuron_EmotionReg/Wager_2008_emo_reg_vs_look_neg_contrast_images.nii
/Users/torwager/Documents/GitHub/CanlabCore/CanlabCore/Sample_datasets/Wager_et_al_2008_Neuron_EmotionReg/Wager_2008_emo_reg_vs_look_neg_contrast_images.nii
/Users/torwager/Documents/GitHub/CanlabCore/CanlabCore/Sample_datasets/Wager_et_al_2008_Neuron_EmotionReg/Wager_2008_emo_reg_vs_look_neg_contrast_images.nii
/Users/torwager/Documents/GitHub/CanlabCore/CanlabCore/Sample_datasets/Wager_et_al_2008_Neuron_EmotionReg/Wager_2008_emo_reg_vs_look_neg_contrast_images.nii
/Users/torwager/Documents/GitHub/CanlabCore/CanlabCore/Sample_datasets/Wager_et_al_2008_Neuron_EmotionReg/Wager_2008_emo_reg_vs_look_neg_contrast_images.nii
/Users/torwager/Documents/GitHub/CanlabCore/CanlabCore/Sample_datasets/Wager_et_al_2008_Neuron_EmotionReg/Wager_2008_emo_reg_vs_look_neg_contrast_images.nii
/Users/torwager/Documents/GitHub/CanlabCore/CanlabCore/Sample_datasets/Wager_et_al_2008_Neuron_EmotionReg/Wager_2008_emo_reg_vs_look_neg_contrast_images.nii
/Users/torwager/Documents/GitHub/CanlabCore/CanlabCore/Sample_datasets/Wager_et_al_2008_Neuron_EmotionReg/Wager_2008_emo_reg_vs_look_neg_contrast_images.nii
/Users/torwager/Documents/GitHub/CanlabCore/CanlabCore/Sample_datasets/Wager_et_al_2008_Neuron_EmotionReg/Wager_2008_emo_reg_vs_look_neg_contrast_images.nii
/Users/torwager/Documents/GitHub/CanlabCore/CanlabCore/Sample_datasets/Wager_et_al_2008_Neuron_EmotionReg/Wager_2008_emo_reg_vs_look_neg_contrast_images.nii
 
Summary of dataset
______________________________________________________
Images:  30	Nonempty:  30	Complete:  30
Voxels: 49792	Nonempty: 49792	Complete: 49245
Unique data values: 1481293
 
Min: -25.515	Max: 15.935	Mean: 0.159	Std: 1.382
 
    Percentiles     Values 
    ___________    ________

        0.1         -8.3774
        0.5          -5.083
          1         -3.9722
          5         -1.8634
         25        -0.39993
         50         0.08903
         75         0.75436
         95          2.2821
         99          4.0183
       99.5          4.8676
       99.9           6.987

 

beh = 

  struct with fields:

          data: [30&times;2 double]
      textdata: {'X_RVLPFC'  'Y_Reappraisal_Success'}
    colheaders: {'X_RVLPFC'  'Y_Reappraisal_Success'}


mask =

    '/Users/torwager/Documents/GitHub/CanlabCore/CanlabCore/canlab_canonical_brains/Canonical_brains_surfaces/gray_matter_mask.img'

</pre><h2 id="2">Visualize the mask</h2><p>--------------------------------------------------------------- BLOCK 2: Check mask</p><pre class="codeinput"><span class="comment">% This is an underlay brain:</span>

o2 = canlab_results_fmridisplay([], <span class="string">'noverbose'</span>);
drawnow, snapnow;

<span class="comment">% This is a basic gray-matter mask we will use for analysis:</span>
<span class="comment">% It can help reduce multiple comparisons relative to whole-image analysis</span>
<span class="comment">% but we should still look at what's happening in ventricles and</span>
<span class="comment">% out-of-brain space to check for artifacts.</span>

o2 = addblobs(o2, region(maskdat));

<span class="comment">% Add a title to montage 1, the axial slice montage:</span>
o2 = title_montage(o2, 1, <span class="string">'Mask image: Gray matter with border'</span>);

drawnow, snapnow;
</pre><img vspace="5" hspace="5" src="canlab_help_5_regression_walkthrough_01.png" alt=""> <pre class="codeoutput">Grouping contiguous voxels:  46 regions
sagittal montage: 4184 voxels displayed, 207155 not displayed on these slices
sagittal montage: 4184 voxels displayed, 207155 not displayed on these slices
sagittal montage: 3961 voxels displayed, 207378 not displayed on these slices
axial montage: 29707 voxels displayed, 181632 not displayed on these slices
axial montage: 32072 voxels displayed, 179267 not displayed on these slices
</pre><img vspace="5" hspace="5" src="canlab_help_5_regression_walkthrough_02.png" alt=""> <h2 id="3">Visualize summary of brain coverage</h2><p>--------------------------------------------------------------- BLOCK 3: Check that we have valid data in most voxels for most subjects</p><pre class="codeinput"><span class="comment">% Show summary of coverage - how many images have non-zero, non-NaN values in each voxel</span>

o2 = removeblobs(o2);
o2 = montage(region(desc.coverage_obj), o2, <span class="string">'trans'</span>, <span class="string">'transvalue'</span>, .3);

o2 = title_montage(o2, 1, <span class="string">'Coverage in images'</span>);

<span class="comment">% OR:</span>
<span class="comment">% orthviews(desc.coverage_obj, 'continuous');</span>
</pre><pre class="codeoutput">Grouping contiguous voxels:   1 regions
</pre><img vspace="5" hspace="5" src="canlab_help_5_regression_walkthrough_03.png" alt=""> <h2 id="4">BLOCK 4</h2><p>--------------------------------------------------------------- Check histograms of individual subjects for global shifts in contrast values</p><pre class="codeinput"><span class="comment">% The 'histogram' object method will allow you to examine a series of</span>
<span class="comment">% images in one panel each.  See help fmri_data.histogram for more options,</span>
<span class="comment">% including breakdown by tissue type.</span>

hist_han = histogram(image_obj, <span class="string">'byimage'</span>, <span class="string">'color'</span>, <span class="string">'b'</span>);

<span class="comment">% This shows us that some of the images do not have the same mean as the</span>
<span class="comment">% others. This is fairly common, as individual subjects can often have</span>
<span class="comment">% global artifacts (e.g., task-correlated head motion or outliers) that</span>
<span class="comment">% influence the whole contrast image, even when baseline conditions are</span>
<span class="comment">% supposed to be "subtracted out".</span>
<span class="comment">%</span>
<span class="comment">% It suggests that we may want to do an outlier analysis and/or standardize</span>
<span class="comment">% the scale of the images. We'll return to this below.</span>
</pre><img vspace="5" hspace="5" src="canlab_help_5_regression_walkthrough_04.png" alt=""> <h2 id="5">BLOCK 5</h2><p>--------------------------------------------------------------- Examine predictor distribution and leverages Leverage is a measure of how much each point influences the regression line. The more extreme the predictor value, the higher the leverage. Outliers will have very high leverage. High-leverage behavioral observations can strongly influence, and sometimes invalidate, an analysis.</p><pre class="codeinput">X = scale(success, 1); X(:, end+1) = 1;         <span class="comment">% A simple design matrix, behavioral predictor + intercept</span>
H = X * inv(X'* X) * X';                        <span class="comment">% The "Hat Matrix", which produces fits. Diagonals are leverage</span>

create_figure(<span class="string">'levs'</span>, 2, 1);
plot(success, <span class="string">'o'</span>, <span class="string">'MarkerFaceColor'</span>, [0 .3 .7], <span class="string">'LineWidth'</span>, 3);
set(gca, <span class="string">'FontSize'</span>, 24);
xlabel(<span class="string">'Subject number'</span>);
ylabel(<span class="string">'Reappraisal success'</span>);

subplot(2, 1, 2);
plot(diag(H), <span class="string">'o'</span>, <span class="string">'MarkerFaceColor'</span>, [0 .3 .7], <span class="string">'LineWidth'</span>, 3);
set(gca, <span class="string">'FontSize'</span>, 24);
xlabel(<span class="string">'Subject number'</span>);
ylabel(<span class="string">'Leverage'</span>);
</pre><img vspace="5" hspace="5" src="canlab_help_5_regression_walkthrough_05.png" alt=""> <h2 id="6">BLOCK 6</h2><p>Run regression The regress method takes predictors that are attached in the object's X attribute (X stands for design matrix) and regresses each voxel's activity (y) on the set of regressors. This is a group analysis, in this case correlating brain activity with reappraisal success at each voxel.</p><pre class="codeinput"><span class="comment">% .X must have the same number of observations, n, in an n x k matrix.</span>
<span class="comment">% n images is the number of COLUMNS in image_obj.dat</span>

<span class="comment">% mean-center success scores and attach them to image_obj in image_obj.X</span>
image_obj.X = scale(success, 1);

<span class="comment">% runs the regression at each voxel and returns statistic info and creates</span>
<span class="comment">% a visual image.  regress = multiple regression.</span>

out = regress(image_obj);

<span class="comment">% out has statistic_image objects that have information about the betas</span>
<span class="comment">% (slopes) b, t-values and p-values (t), degrees of freedom (df), and sigma</span>
<span class="comment">% (error variance).  The critical one is out.t.</span>
<span class="comment">% out =</span>
<span class="comment">%</span>
<span class="comment">%         b: [1x1 statistic_image]</span>
<span class="comment">%         t: [1x1 statistic_image]</span>
<span class="comment">%        df: [1x1 fmri_data]</span>
<span class="comment">%     sigma: [1x1 fmri_data]</span>

<span class="comment">% Now let's try thresholding the image at q &lt; .05 fdr-corrected.</span>
 t = threshold(out.t, .05, <span class="string">'fdr'</span>);

<span class="comment">% ...and display</span>

o2 = montage(t, <span class="string">'trans'</span>);
o2 = title_montage(o2, 2, <span class="string">'Behavioral predictor: Reappraisal success'</span>);
o2 = title_montage(o2, 4, <span class="string">'Intercept: Group average activation'</span>);
snapnow

<span class="comment">% Or:</span>
<span class="comment">% orthviews(t)</span>

<span class="comment">% This is a multiple regression, and there are two output t images, one for</span>
<span class="comment">% each regressor.  We've only entered one regressor, why two images?  The program always</span>
<span class="comment">% adds an intercept by default.  The intercept is always the last column of the design matrix</span>

<span class="comment">% Image   1  &lt;--- brain contrast values correlated with "success"</span>
<span class="comment">% Positive effect:   0 voxels, min p-value: 0.00001192</span>
<span class="comment">% Negative effect:   0 voxels, min p-value: 0.00170529</span>
<span class="comment">% Image   2 FDR q &lt; 0.050 threshold is 0.003193</span>
<span class="comment">%</span>
<span class="comment">% Image   2 &lt;--- intercept, because we have mean-centered, this is the</span>
<span class="comment">% average group effect (when success = average success).  "Reapp - Look"</span>
<span class="comment">% contrast in the whole group.</span>
<span class="comment">% Positive effect: 3133 voxels, min p-value: 0.00000000</span>
<span class="comment">% Negative effect:  51 voxels, min p-value: 0.00024068</span>

<span class="comment">% re-threshold at p &lt; .005 uncorrected</span>
t = threshold(out.t, .005, <span class="string">'unc'</span>);

o2 = montage(t, <span class="string">'trans'</span>);
o2 = title_montage(o2, 2, <span class="string">'Behavioral predictor: Reappraisal success'</span>);
o2 = title_montage(o2, 4, <span class="string">'Intercept: Group average activation'</span>);
snapnow
</pre><pre class="codeoutput">Analysis: 
----------------------------------
Design matrix warnings:
----------------------------------
No intercept detected, adding intercept to last column of design matrix                             
Warning:  Predictors are not centered -- intercept is not interpretable as stats for average subject
 
----------------------------------
Running regression: 49792 voxels. Design:  30 obs,   2 regressors, intercept is last

Predicting exogenous variable(s) in dat.X using brain data as predictors, mass univariate
Running in OLS Mode
Model run in 0 minutes and 0.04 seconds

Image   1
 24 contig. clusters, sizes   1 to  51
Positive effect: 203 voxels, min p-value: 0.00001192
Negative effect:   0 voxels, min p-value: 0.00170529

Image   2
 24 contig. clusters, sizes   1 to  51
Positive effect: 1969 voxels, min p-value: 0.00000000
Negative effect:  11 voxels, min p-value: 0.00024068

SPM12: spm_check_registration (v6245)              01:44:04 - 30/07/2018
========================================================================
Display &lt;a href="matlab:spm_image('display','/Users/torwager/Documents/GitHub/CanlabCore/CanlabCore/canlab_canonical_brains/Canonical_brains_surfaces/keuken_2014_enhanced_for_underlay.img,1');"&gt;/Users/torwager/Documents/GitHub/CanlabCore/CanlabCore/canlab_canonical_brains/Canonical_brains_surfaces/keuken_2014_enhanced_for_underlay.img,1&lt;/a&gt;
 (&lt;a href="matlab:spm_check_registration('/Users/torwager/Documents/GitHub/CanlabCore/CanlabCore/canlab_canonical_brains/Canonical_brains_surfaces/keuken_2014_enhanced_for_underlay.img,1','/Users/torwager/Documents/GitHub/CanlabCore/CanlabCore/canlab_canonical_brains/Canonical_brains_surfaces/keuken_2014_enhanced_for_underlay.img,1');"&gt;all&lt;/a&gt;)  &lt;a href="matlab:spm_image('display','/Users/torwager/Documents/GitHub/CanlabCore/CanlabCore/canlab_canonical_brains/Canonical_brains_surfaces/keuken_2014_enhanced_for_underlay.img,1');"&gt;/Users/torwager/Documents/GitHub/CanlabCore/CanlabCore/canlab_canonical_brains/Canonical_brains_surfaces/keuken_2014_enhanced_for_underlay.img,1&lt;/a&gt;
Image   1 FDR q &lt; 0.050 threshold is 0.000000

Image   1
  0 contig. clusters, sizes  to 
Positive effect:   0 voxels, min p-value: 0.00001192
Negative effect:   0 voxels, min p-value: 0.00170529
Image   2 FDR q &lt; 0.050 threshold is 0.003193

Image   2
  0 contig. clusters, sizes  to 
Positive effect: 3133 voxels, min p-value: 0.00000000
Negative effect:  51 voxels, min p-value: 0.00024068
Setting up fmridisplay objects
This takes a lot of memory, and can hang if you have too little.
Grouping contiguous voxels:   0 regions
Grouping contiguous voxels:  28 regions
sagittal montage: 162 voxels displayed, 3022 not displayed on these slices
axial montage: 1617 voxels displayed, 1567 not displayed on these slices
</pre><img vspace="5" hspace="5" src="canlab_help_5_regression_walkthrough_06.png" alt=""> <img vspace="5" hspace="5" src="canlab_help_5_regression_walkthrough_07.png" alt=""> <pre class="codeoutput">
Image   1
 67 contig. clusters, sizes   1 to 255
Positive effect: 1159 voxels, min p-value: 0.00001192
Negative effect:  10 voxels, min p-value: 0.00170529

Image   2
 67 contig. clusters, sizes   1 to 255
Positive effect: 3673 voxels, min p-value: 0.00000000
Negative effect:  75 voxels, min p-value: 0.00024068
Setting up fmridisplay objects
This takes a lot of memory, and can hang if you have too little.
Grouping contiguous voxels:  67 regions
sagittal montage: 119 voxels displayed, 1050 not displayed on these slices
axial montage: 439 voxels displayed, 730 not displayed on these slices
Grouping contiguous voxels:  33 regions
sagittal montage: 191 voxels displayed, 3557 not displayed on these slices
axial montage: 1921 voxels displayed, 1827 not displayed on these slices
</pre><img vspace="5" hspace="5" src="canlab_help_5_regression_walkthrough_08.png" alt=""> <h2 id="7">BLOCK 7</h2><p>Display the results on slices</p><pre class="codeinput"><span class="comment">% multi_threshold lets us see the blobs with significant voxels at the</span>
<span class="comment">% highest (most stringent) threshold, and voxels that are touching</span>
<span class="comment">% (contiguous) down to the lowest threshold, in different colors.</span>

o2 = multi_threshold(out.t, <span class="string">'thresh'</span>, [.005 .01 .05], <span class="string">'sizethresh'</span>, [5 1 1]);

o2 = title_montage(o2, 2, <span class="string">'Behavioral predictor: Reappraisal success'</span>);
o2 = title_montage(o2, 4, <span class="string">'Intercept: Group average activation'</span>);
snapnow
</pre><pre class="codeoutput">
Multi-threshold
_____________________________________________
Retaining clusters contiguous with a significant cluster at p &lt; 0.00500000 and k &gt;=   5

Image   1 of   2
_____________________________________________
Threshold p &lt; 0.00500000 and k &gt;=   5	1093 significant voxels defining blobs
Threshold p &lt; 0.01000000 and k &gt;=   1	2167 significant voxels contiguous with a significant blob at a higher threshold
Threshold p &lt; 0.05000000 and k &gt;=   1	9039 significant voxels contiguous with a significant blob at a higher threshold

Image   2 of   2
_____________________________________________
Threshold p &lt; 0.00500000 and k &gt;=   5	3714 significant voxels defining blobs
Threshold p &lt; 0.01000000 and k &gt;=   1	4838 significant voxels contiguous with a significant blob at a higher threshold
Threshold p &lt; 0.05000000 and k &gt;=   1	8710 significant voxels contiguous with a significant blob at a higher threshold


Warning: Mask has multiple images, will use first only.
Warning: Mask has multiple images, will use first only.
Warning: Mask has multiple images, will use first only.
Setting up fmridisplay objects
This takes a lot of memory, and can hang if you have too little.

Montage 1 of 2
_____________________________________
</pre><img vspace="5" hspace="5" src="canlab_help_5_regression_walkthrough_09.png" alt=""> <pre class="codeoutput">
Montage 2 of 2
_____________________________________
</pre><img vspace="5" hspace="5" src="canlab_help_5_regression_walkthrough_10.png" alt=""> <img vspace="5" hspace="5" src="canlab_help_5_regression_walkthrough_11.png" alt=""> <h2 id="8">BLOCK 8: Look for signal in ventricles, white matter, outside of brain</h2><p>We want to diagnose potential problems due to outliers, etc...</p><pre class="codeinput"><span class="comment">% Strategy 1:  Apply a very liberal threshold.</span>
t = threshold(out.t, .05, <span class="string">'unc'</span>);

figure;
o2 = canlab_results_fmridisplay;
o2 = addblobs(o2, region(t));

o2 = title_montage(o2, 5, <span class="string">'Liberal threshold: .05 uncorrected'</span>);


<span class="comment">% Strategy 2: Extract mean signal from WM and ventricles</span>
m = extract_gray_white_csf(image_obj);

barplot_columns(m, <span class="string">'title'</span>, <span class="string">'Gray white CSF per subject'</span>, <span class="string">'names'</span>, {<span class="string">'Gray'</span> <span class="string">'White'</span> <span class="string">'CSF'</span>}, <span class="string">'XTick'</span>, [1:3], <span class="string">'colors'</span>, {[1 0 0] [0 .7 0] [0 0 1]});

ylabel(<span class="string">'Contrast values'</span>);


<span class="comment">% global_gray_white_csf = extract_gray_white_csf(image_obj);</span>
<span class="comment">% corr([global_gray_white_csf diag(H) success])</span>
</pre><pre class="codeoutput">
Image   1
 62 contig. clusters, sizes   1 to 8922
Positive effect: 9098 voxels, min p-value: 0.00001192
Negative effect: 188 voxels, min p-value: 0.00170529

Image   2
 62 contig. clusters, sizes   1 to 8922
Positive effect: 8577 voxels, min p-value: 0.00000000
Negative effect: 955 voxels, min p-value: 0.00024068
Setting up fmridisplay objects
This takes a lot of memory, and can hang if you have too little.
Warning: Mask has multiple images, will use first only.
Grouping contiguous voxels:  62 regions
sagittal montage: 397 voxels displayed, 8889 not displayed on these slices
sagittal montage: 487 voxels displayed, 8799 not displayed on these slices
sagittal montage: 486 voxels displayed, 8800 not displayed on these slices
axial montage: 2768 voxels displayed, 6518 not displayed on these slices
axial montage: 2918 voxels displayed, 6368 not displayed on these slices
Extracting from gray_matter_mask_sparse.img.
Extracting from canonical_white_matter.img.
Extracting from canonical_ventricles.img.
Col   1: Gray	Col   2: White	Col   3: CSF	
---------------------------------------------
Tests of column means against zero
---------------------------------------------
     Name      Mean_Value    Std_Error       T          P       Cohens_d
    _______    __________    _________    _______    _______    ________

    'Gray'       0.21392      0.17152      1.2472     0.2223     0.22771
    'White'     0.056209      0.10298     0.54584    0.58935    0.099657
    'CSF'       -0.24914      0.18332     -1.3591     0.1846    -0.24813

</pre><img vspace="5" hspace="5" src="canlab_help_5_regression_walkthrough_12.png" alt=""> <img vspace="5" hspace="5" src="canlab_help_5_regression_walkthrough_13.png" alt=""> <h2 id="9">BLOCK 9: Compare results to meta-analysis for positive controls</h2><pre class="codeinput"><span class="comment">% Map from neurosynth.org</span>
metaimg = which(<span class="string">'emotion regulation_pAgF_z_FDR_0.01_8_14_2015.nii'</span>)
r = region(metaimg);

o2 = removeblobs(o2);
o2 = addblobs(o2, r, <span class="string">'maxcolor'</span>, [1 0 0], <span class="string">'mincolor'</span>, [.7 .2 .5]);

o2 = title_montage(o2, 5, <span class="string">'Neurosynth mask: Emotion regulation'</span>);
</pre><pre class="codeoutput">
metaimg =

    '/Users/torwager/Documents/GitHub/MediationToolbox/Mediation_walkthrough/emotion regulation_pAgF_z_FDR_0.01_8_14_2015.nii'

Grouping contiguous voxels: 151 regions
sagittal montage: 186 voxels displayed, 5855 not displayed on these slices
sagittal montage: 139 voxels displayed, 5902 not displayed on these slices
sagittal montage: 204 voxels displayed, 5837 not displayed on these slices
axial montage: 1118 voxels displayed, 4923 not displayed on these slices
axial montage: 1009 voxels displayed, 5032 not displayed on these slices
</pre><img vspace="5" hspace="5" src="canlab_help_5_regression_walkthrough_14.png" alt=""> <h2 id="10">BLOCK 10: Apply gray-matter mask and show FDR-thresholded results</h2><pre class="codeinput"><span class="comment">% This can increase power by focusing on areas we think there are plausible</span>
<span class="comment">% effects. First is the success effect (regressor) and second is the</span>
<span class="comment">% intercept (group average contrast) map.</span>

<span class="comment">%t = threshold(out.t, .05, 'fdr', 'mask', mask);</span>

t = apply_mask(out.t, maskdat);
t = threshold(t, .05, <span class="string">'fdr'</span>);

<span class="comment">% Select the reappraisal success effect only and show it</span>
t1 = select_one_image(t, 1);

o2 = removeblobs(o2);
o2 = addblobs(o2, region(t1));

o2 = title_montage(o2, 5, <span class="string">'Behavioral predictor, gray-matter masked, q &lt; .05 FDR'</span>);
</pre><pre class="codeoutput">Image   1 FDR q &lt; 0.050 threshold is 0.000000

Image   1
  0 contig. clusters, sizes  to 
Positive effect:   0 voxels, min p-value: 0.00002897
Negative effect:   0 voxels, min p-value: 0.00847960
Image   2 FDR q &lt; 0.050 threshold is 0.002407

Image   2
  0 contig. clusters, sizes  to 
Positive effect: 2370 voxels, min p-value: 0.00000000
Negative effect:  28 voxels, min p-value: 0.00041628
Grouping contiguous voxels:   0 regions
</pre><img vspace="5" hspace="5" src="canlab_help_5_regression_walkthrough_15.png" alt=""> <h2 id="11">BLOCK 11: Refine analysis by removing outlier and ranking predictor values</h2><pre class="codeinput"><span class="comment">% exclude high-leverage subject 16</span>
datno16 = image_obj;
datno16.dat(:, 16) = [];

<span class="comment">% try rank: robust...</span>
<span class="comment">% Ranking is a kind of nonparametric</span>

datno16.X = success;
datno16.X(16) = [];
datno16.X = scale(rankdata(datno16.X), 1);

<span class="comment">% Re-run regression</span>
out = regress(datno16);

<span class="comment">% Apply gray matter mask and threshold</span>
t = apply_mask(out.t, maskdat);
t = threshold(t, .005, <span class="string">'unc'</span>);
<span class="comment">%orthviews(t)</span>

o2 = multi_threshold(t, <span class="string">'thresh'</span>, [.005 .01 .05], <span class="string">'sizethresh'</span>, [1 1 1]);

o2 = title_montage(o2, 2, <span class="string">'Behavioral predictor: Reappraisal success'</span>);
o2 = title_montage(o2, 4, <span class="string">'Intercept: Group average activation'</span>);
snapnow


<span class="comment">% Select the reappraisal success effect only and write it to disk</span>

t1 = select_one_image(t, 1);
t1.fullpath = fullfile(pwd, <span class="string">'Reapp_Success_005_outlier_removed.nii'</span>);
write(t1)
</pre><pre class="codeoutput">Analysis: 
----------------------------------
Design matrix warnings:
----------------------------------
No intercept detected, adding intercept to last column of design matrix
 
----------------------------------
Running regression: 49792 voxels. Design:  29 obs,   2 regressors, intercept is last

Predicting exogenous variable(s) in dat.X using brain data as predictors, mass univariate
Running in OLS Mode
Model run in 0 minutes and 0.04 seconds

Image   1
  2 contig. clusters, sizes   1 to   5
Positive effect:   1 voxels, min p-value: 0.00078094
Negative effect:   5 voxels, min p-value: 0.00054371

Image   2
  2 contig. clusters, sizes   1 to   5
Positive effect: 3604 voxels, min p-value: 0.00000000
Negative effect:   5 voxels, min p-value: 0.00048983

SPM12: spm_check_registration (v6245)              01:44:52 - 30/07/2018
========================================================================
Display &lt;a href="matlab:spm_image('display','/Users/torwager/Documents/GitHub/CanlabCore/CanlabCore/canlab_canonical_brains/Canonical_brains_surfaces/keuken_2014_enhanced_for_underlay.img,1');"&gt;/Users/torwager/Documents/GitHub/CanlabCore/CanlabCore/canlab_canonical_brains/Canonical_brains_surfaces/keuken_2014_enhanced_for_underlay.img,1&lt;/a&gt;
 (&lt;a href="matlab:spm_check_registration('/Users/torwager/Documents/GitHub/CanlabCore/CanlabCore/canlab_canonical_brains/Canonical_brains_surfaces/keuken_2014_enhanced_for_underlay.img,1','/Users/torwager/Documents/GitHub/CanlabCore/CanlabCore/canlab_canonical_brains/Canonical_brains_surfaces/keuken_2014_enhanced_for_underlay.img,1');"&gt;all&lt;/a&gt;)  &lt;a href="matlab:spm_image('display','/Users/torwager/Documents/GitHub/CanlabCore/CanlabCore/canlab_canonical_brains/Canonical_brains_surfaces/keuken_2014_enhanced_for_underlay.img,1');"&gt;/Users/torwager/Documents/GitHub/CanlabCore/CanlabCore/canlab_canonical_brains/Canonical_brains_surfaces/keuken_2014_enhanced_for_underlay.img,1&lt;/a&gt;

Image   1
 12 contig. clusters, sizes   1 to  15
Positive effect:  40 voxels, min p-value: 0.00117099
Negative effect:   3 voxels, min p-value: 0.00268126

Image   2
 12 contig. clusters, sizes   1 to  15
Positive effect: 4940 voxels, min p-value: 0.00000000
Negative effect:  32 voxels, min p-value: 0.00082040

Multi-threshold
_____________________________________________
Retaining clusters contiguous with a significant cluster at p &lt; 0.00500000 and k &gt;=   1

Image   1 of   2
_____________________________________________
Threshold p &lt; 0.00500000 and k &gt;=   1	 43 significant voxels defining blobs
Threshold p &lt; 0.01000000 and k &gt;=   1	141 significant voxels contiguous with a significant blob at a higher threshold
Threshold p &lt; 0.05000000 and k &gt;=   1	801 significant voxels contiguous with a significant blob at a higher threshold

Image   2 of   2
_____________________________________________
Threshold p &lt; 0.00500000 and k &gt;=   1	4972 significant voxels defining blobs
Threshold p &lt; 0.01000000 and k &gt;=   1	6056 significant voxels contiguous with a significant blob at a higher threshold
Threshold p &lt; 0.05000000 and k &gt;=   1	10008 significant voxels contiguous with a significant blob at a higher threshold


Warning: Mask has multiple images, will use first only.
Warning: Mask has multiple images, will use first only.
Warning: Mask has multiple images, will use first only.
Setting up fmridisplay objects
This takes a lot of memory, and can hang if you have too little.

Montage 1 of 2
_____________________________________
</pre><img vspace="5" hspace="5" src="canlab_help_5_regression_walkthrough_16.png" alt=""> <img vspace="5" hspace="5" src="canlab_help_5_regression_walkthrough_17.png" alt=""> <pre class="codeoutput">
Montage 2 of 2
_____________________________________
</pre><img vspace="5" hspace="5" src="canlab_help_5_regression_walkthrough_18.png" alt=""> <img vspace="5" hspace="5" src="canlab_help_5_regression_walkthrough_19.png" alt=""> <pre class="codeoutput">Writing: 
/Users/torwager/Documents/GitHub/Reapp_Success_005_outlier_removed.nii
</pre><h2 id="12">Block 12: Extract and plot data from (biased) regions of interest</h2><p>Let's visualize the correlation scatterplots in the areas we've discovered as related to Success</p><pre class="codeinput"><span class="comment">% Select the Success regressor map</span>
r = region(t1);

<span class="comment">% Autolabel regions and print a table</span>
r = table(r);

<span class="comment">% Make a montage showing each significant region</span>
montage(r, <span class="string">'colormap'</span>, <span class="string">'regioncenters'</span>);
</pre><pre class="codeoutput">Grouping contiguous voxels:  12 regions

____________________________________________________________________________________________________________________________________________
Positive Effects
        Region        Volume           XYZ            maxZ      modal_label_descriptions      Perc_covered_by_label    Atlas_regions_covered    region_index
    ______________    ______    _________________    ______    ___________________________    _____________________    _____________________    ____________

    'V_Striatum_L'     3120      -7     21     -9    3.0688    'Basal_ganglia'                         22                        6                    5     
    'Cau_L'             288     -21      7     23    2.9042    'Basal_ganglia'                         61                        0                    7     
    'Ctx_RSC_R'         736       3    -41      5    2.9848    'Cortex_Default_ModeA'                  34                        1                    6     
    'Ctx_SFL_L'         512     -14     28     63    2.8084    'Cortex_Default_ModeB'                  66                        1                    9     
    'Ctx_PHA3_L'       1296     -31    -31    -18    3.1243    'Cortex_Default_ModeC'                  28                        3                    3     
    'Ctx_TE1m_L'        640     -69    -31    -23    3.2459    'Cortex_Fronto_ParietalB'               24                        0                    4     
    'Ctx_TE2a_R'        640      52    -21    -32    2.9725    'Cortex_Limbic'                         57                        1                    2     
    'Ctx_6d_R'          704      38     -7     68    2.9676    'Cortex_SomatomotorA'                   51                        1                   10     
    'Ctx_6ma_R'        2048      14      3     63    3.2022    'Cortex_Ventral_AttentionA'             18                        2                    8     
    'Ctx_6ma_R'         512      24      0     68     2.851    'Cortex_Ventral_AttentionA'             81                        1                   11     
    'No label'          384     -28    -17    -41    2.8133    'X_No_label'                             0                        0                    1     


Negative Effects
      Region       Volume          XYZ            maxZ       modal_label_descriptions     Perc_covered_by_label    Atlas_regions_covered    region_index
    ___________    ______    ________________    _______    __________________________    _____________________    _____________________    ____________

    'Ctx_STV_R'     768      52    -48     18    -3.0021    'Cortex_Temporal_Parietal'             61                        1                   12     


____________________________________________________________________________________________________________________________________________
Regions labeled by reference atlas CANlab_2018_combined
                                                       
Volume: Volume of contiguous region in cubic mm.
MaxZ: Signed max over Z-score for each voxel.
Atlas_regions_covered: Number of reference atlas regions covered at least 5% by the region. This relates to whether the region covers 
multiple reference atlas regions                                                                                                      
Region: Best reference atlas label, defined as reference region with highest number of in-region voxels.
Perc_covered_by_label: Percentage of the region covered by the label.
Ref_region_perc: Percentage of the label region within the target region.
modal_atlas_index: Index number of label region in reference atlas
 
For example, if a region is labeled 'TE1a' and Perc_covered_by_label = 8, Ref_region_perc = 38, and Atlas_regions_covered = 17, this means 
that 8% of the region's voxels are labeled TE1a, which is the highest percentage among reference label regions. 38% of the region TE1a is  
covered by the region. However, the region covers at least 5% of 17 distinct labeled reference regions.                                    
                                                                                                                                           
References for atlases:
                       
Beliveau, Vincent, Claus Svarer, Vibe G. Frokjaer, Gitte M. Knudsen, Douglas N. Greve, and Patrick M. Fisher. 2015. &#8220;Functional 
Connectivity of the Dorsal and Median Raphe Nuclei at Rest.&#8221; NeuroImage 116 (August): 187&#8211;95.                                   
B&auml;r, Karl-J&uuml;rgen, Feliberto de la Cruz, Andy Schumann, Stefanie Koehler, Heinrich Sauer, Hugo Critchley, and Gerd Wagner. 2016. ?Functional 
Connectivity and Network Analysis of Midbrain and Brainstem Nuclei.? NeuroImage 134 (July):53?63.                                           
Diedrichsen, J&ouml;rn, Joshua H. Balsters, Jonathan Flavell, Emma Cussans, and Narender Ramnani. 2009. A Probabilistic MR Atlas of the Human 
Cerebellum. NeuroImage 46 (1): 39?46.                                                                                                    
Fairhurst, Merle, Katja Wiech, Paul Dunckley, and Irene Tracey. 2007. ?Anticipatory Brainstem Activity Predicts Neural Processing of Pain 
in Humans.? Pain 128 (1-2):101?10.                                                                                                        
Fan 2016 Cerebral Cortex; doi:10.1093/cercor/bhw157
Glasser, Matthew F., Timothy S. Coalson, Emma C. Robinson, Carl D. Hacker, John Harwell, Essa Yacoub, Kamil Ugurbil, et al. 2016. A 
Multi-Modal Parcellation of Human Cerebral Cortex. Nature 536 (7615): 171?78.                                                       
Keren, Noam I., Carl T. Lozar, Kelly C. Harris, Paul S. Morgan, and Mark A. Eckert. 2009. &#8220;In Vivo Mapping of the Human Locus Coeruleus.&#8221; 
NeuroImage 47 (4): 1261&#8211;67.                                                                                                               
Keuken, M. C., P-L Bazin, L. Crown, J. Hootsmans, A. Laufer, C. M&uuml;ller-Axt, R. Sier, et al. 2014. &#8220;Quantifying Inter-Individual Anatomical 
Variability in the Subcortex Using 7 T Structural MRI.&#8221; NeuroImage 94 (July): 40&#8211;46.                                                       
Krauth A, Blanc R, Poveda A, Jeanmonod D, Morel A, Sz&eacute;kely G. (2010) A mean three-dimensional atlas of the human thalamus: generation from 
multiple histological data. Neuroimage. 2010 Feb 1;49(3):2053-62. Jakab A, Blanc R, Ber&eacute;nyi EL, Sz&eacute;kely G. (2012) Generation of            
Individualized Thalamus Target Maps by Using Statistical Shape Models and Thalamocortical Tractography. AJNR Am J Neuroradiol. 33:         
2110-2116, doi: 10.3174/ajnr.A3140                                                                                                         
Nash, Paul G., Vaughan G. Macefield, Iven J. Klineberg, Greg M. Murray, and Luke A. Henderson. 2009. ?Differential Activation of the Human 
Trigeminal Nuclear Complex by Noxious and Non-Noxious Orofacial Stimulation.? Human Brain Mapping 30 (11):3772?82.                         
Pauli 2018 Bioarxiv: CIT168 from Human Connectome Project data
Pauli, Wolfgang M., Amanda N. Nili, and J. Michael Tyszka. 2018. ?A High-Resolution Probabilistic in Vivo Atlas of Human Subcortical Brain 
Nuclei.? Scientific Data 5 (April): 180063.                                                                                                
Pauli, Wolfgang M., Randall C. O?Reilly, Tal Yarkoni, and Tor D. Wager. 2016. ?Regional Specialization within the Human Striatum for 
Diverse Psychological Functions.? Proceedings of the National Academy of Sciences of the United States of America 113 (7): 1907?12.  
Sclocco, Roberta, Florian Beissner, Gaelle Desbordes, Jonathan R. Polimeni, Lawrence L. Wald, Norman W. Kettner, Jieun Kim, et al. 2016. 
?Neuroimaging Brainstem Circuitry Supporting Cardiovagal Response to Pain: A Combined Heart Rate Variability/ultrahigh-Field (7 T)       
Functional Magnetic Resonance Imaging Study.? Philosophical Transactions. Series A, Mathematical, Physical, and Engineering Sciences 374 
(2067). rsta.royalsocietypublishing.org. https://doi.org/10.1098/rsta.2015.0189.                                                         
Shen, X., F. Tokoglu, X. Papademetris, and R. T. Constable. 2013. &#8220;Groupwise Whole-Brain Parcellation from Resting-State fMRI Data for 
Network Node Identification.&#8221; NeuroImage 82 (November): 403&#8211;15.                                                                        
Zambreanu, L., R. G. Wise, J. C. W. Brooks, G. D. Iannetti, and I. Tracey. 2005. ?A Role for the Brainstem in Central Sensitisation in 
Humans. Evidence from Functional Magnetic Resonance Imaging.? Pain 114 (3):397?407.                                                    
____________________________________________________________________________________________________________________________________________
</pre><img vspace="5" hspace="5" src="canlab_help_5_regression_walkthrough_20.png" alt=""> <h2 id="13">Block 13: Extract and plot data from (biased) regions of interest</h2><p>Let's visualize the correlation scatterplots in the areas we've discovered as related to Success</p><pre class="codeinput"><span class="comment">% Extract data from all regions</span>
<span class="comment">% r(i).dat has the averages for each subject across voxels for region i</span>
r = extract_data(r, datno16);

<span class="comment">% Select only regions with 3+ voxels</span>
wh = cat(1, r.numVox) &lt; 3;
r(wh) = [];

<span class="comment">% Set up the scatterplots</span>
nrows = floor(sqrt(length(r)));
ncols = ceil(length(r) ./ nrows);
create_figure(<span class="string">'scatterplot_region'</span>, nrows, ncols);

<span class="comment">% Make a loop and plot each region</span>
<span class="keyword">for</span> i = 1:length(r)

    subplot(nrows, ncols, i);

    <span class="comment">% Use this line for non-robust correlations:</span>
    <span class="comment">%plot_correlation_samefig(r(i).dat, datno16.X);</span>

    <span class="comment">% Use this line for robust correlations:</span>
    plot_correlation_samefig(r(i).dat, datno16.X, [], <span class="string">'k'</span>, 0, 1);

    set(gca, <span class="string">'FontSize'</span>, 12);
    xlabel(<span class="string">'Reappraise - Look Neg brain response'</span>);
    ylabel(<span class="string">'Reappraisal success'</span>);

    <span class="comment">% input('Press a key to continue');</span>

<span class="keyword">end</span>
</pre><img vspace="5" hspace="5" src="canlab_help_5_regression_walkthrough_21.png" alt=""> <h2 id="14">Block 14: Extract and plot data from unbiased regions of interest</h2><p>Let's visualize the correlation scatterplots in some meta-analysis derived ROIs</p><pre class="codeinput"><span class="comment">% Select the Neurosynth meta-analysis map</span>
r = region(metaimg);

<span class="comment">% Extract data from all regions</span>
r = extract_data(r, datno16);

<span class="comment">% Select only regions with 20+ voxels</span>
wh = cat(1, r.numVox) &lt; 20;
r(wh) = [];

r = table(r);

<span class="comment">% Make a montage showing each significant region</span>
montage(r, <span class="string">'colormap'</span>, <span class="string">'regioncenters'</span>);

<span class="comment">% Set up the scatterplots</span>
nrows = floor(sqrt(length(r)));
ncols = ceil(length(r) ./ nrows);
create_figure(<span class="string">'scatterplot_region'</span>, nrows, ncols);

<span class="comment">% Make a loop and plot each region</span>
<span class="keyword">for</span> i = 1:length(r)

    subplot(nrows, ncols, i);

    <span class="comment">% Use this line for non-robust correlations:</span>
    <span class="comment">%plot_correlation_samefig(r(i).dat, datno16.X);</span>

    <span class="comment">% Use this line for robust correlations:</span>
    plot_correlation_samefig(r(i).dat, datno16.X, [], <span class="string">'k'</span>, 0, 1);

    set(gca, <span class="string">'FontSize'</span>, 12);
    xlabel(<span class="string">'Brain'</span>);
    ylabel(<span class="string">'Success'</span>);
    title(<span class="string">' '</span>);

<span class="keyword">end</span>
</pre><pre class="codeoutput">Grouping contiguous voxels: 151 regions

____________________________________________________________________________________________________________________________________________
Positive Effects
        Region        Volume           XYZ            maxZ      modal_label_descriptions      Perc_covered_by_label    Atlas_regions_covered    region_index
    ______________    ______    _________________    ______    ___________________________    _____________________    _____________________    ____________

    'Amygdala_LB_'     4624     -22     -4    -16    7.0345    'Amygdala'                              24                       11                    1     
    'Amygdala_LB_'     3664      24     -4    -16    7.0345    'Amygdala'                              36                        8                    2     
    'Caudate_Ca_L'      208     -14      6     10    5.4136    'Basal_ganglia'                         73                        0                   14     
    'Bstem_SC_L'        560      -4    -28     -4    7.0345    'Brainstem'                             44                        4                    9     
    'Ctx_10v_R'        1192      -2     52     -8    7.0345    'Cortex_Default_ModeA'                  32                        3                    6     
    'Ctx_9m_R'          160       0     62     18    4.7765    'Cortex_Default_ModeA'                  65                        0                   16     
    'Ctx_d23ab_L'       408      -2    -52     28    6.0507    'Cortex_Default_ModeA'                  35                        2                   20     
    'Ctx_9m_R'          296       4     56     32    6.0507    'Cortex_Default_ModeA'                  84                        0                   26     
    'Ctx_STSdp_L'       632     -58    -36     -2    7.0345    'Cortex_Default_ModeB'                  43                        2                    8     
    'Ctx_44_R'         1096      52     22     24    7.0345    'Cortex_Default_ModeB'                  39                        3                   17     
    'Ctx_PGi_L'        1048     -52    -58     28    7.0345    'Cortex_Default_ModeB'                  50                        1                   18     
    'Ctx_8Av_L'        1400     -40     12     46    7.0345    'Cortex_Default_ModeB'                  23                        2                   29     
    'Ctx_FFC_L'         272     -42    -48    -18    6.6878    'Cortex_Dorsal_AttentionA'              76                        1                    3     
    'Ctx_TPOJ2_L'       640     -52    -66      8    6.0507    'Cortex_Dorsal_AttentionA'              55                        2                   13     
    'Ctx_PFop_L'        272     -62    -28     32    6.6878    'Cortex_Dorsal_AttentionB'              50                        1                   24     
    'Ctx_IFJa_L'        528     -44     14     30    7.0345    'Cortex_Fronto_ParietalA'               52                        2                   22     
    'Ctx_a47r_L'        360     -42     46    -10    7.0345    'Cortex_Fronto_ParietalB'               62                        0                    7     
    'Ctx_8BM_R'        6872       0     18     44    7.0345    'Cortex_Fronto_ParietalB'               21                       13                   19     
    'Ctx_PFm_R'         984      52    -50     42    7.0345    'Cortex_Fronto_ParietalB'               79                        1                   27     
    'Ctx_8Av_R'         544      38     24     40    5.4136    'Cortex_Fronto_ParietalB'               38                        1                   28     
    'Ctx_PFm_L'         344     -50    -52     42    6.0507    'Cortex_Fronto_ParietalB'               58                        0                   30     
    'Ctx_PSL_R'         392      58    -44     28    6.0507    'Cortex_Temporal_Parietal'              63                        1                   21     
    'Ctx_MI_L'        10992     -42     22     -2    7.0345    'Cortex_Ventral_AttentionA'             14                       12                    5     
    'Ctx_6r_R'          456      48      6     30    7.0345    'Cortex_Ventral_AttentionA'             46                        1                   23     
    'Ctx_AVI_R'        4624      38     20     -4    7.0345    'Cortex_Ventral_AttentionB'             22                        8                    4     
    'Ctx_IFSa_R'        528      52     30      8    6.0507    'Cortex_Ventral_AttentionB'             38                        2                   12     
    'Ctx_9_46d_L'       168     -32     50     14    5.4136    'Cortex_Ventral_AttentionB'             62                        0                   15     
    'Ctx_46_R'          232      34     40     30    5.4136    'Cortex_Ventral_AttentionB'             72                        1                   25     
    'Thal_LGN'          280     -22    -26     -6    5.4136    'Diencephalon'                          37                        2                   10     
    'Thal_MD'          1696      -2    -14      4    7.0345    'Diencephalon'                          57                        3                   11     


Negative Effects
No regions to display

____________________________________________________________________________________________________________________________________________
Regions labeled by reference atlas CANlab_2018_combined
                                                       
Volume: Volume of contiguous region in cubic mm.
MaxZ: Signed max over Input mask image values for each voxel.
Atlas_regions_covered: Number of reference atlas regions covered at least 5% by the region. This relates to whether the region covers 
multiple reference atlas regions                                                                                                      
Region: Best reference atlas label, defined as reference region with highest number of in-region voxels.
Perc_covered_by_label: Percentage of the region covered by the label.
Ref_region_perc: Percentage of the label region within the target region.
modal_atlas_index: Index number of label region in reference atlas
 
For example, if a region is labeled 'TE1a' and Perc_covered_by_label = 8, Ref_region_perc = 38, and Atlas_regions_covered = 17, this means 
that 8% of the region's voxels are labeled TE1a, which is the highest percentage among reference label regions. 38% of the region TE1a is  
covered by the region. However, the region covers at least 5% of 17 distinct labeled reference regions.                                    
                                                                                                                                           
References for atlases:
                       
Beliveau, Vincent, Claus Svarer, Vibe G. Frokjaer, Gitte M. Knudsen, Douglas N. Greve, and Patrick M. Fisher. 2015. &#8220;Functional 
Connectivity of the Dorsal and Median Raphe Nuclei at Rest.&#8221; NeuroImage 116 (August): 187&#8211;95.                                   
B&auml;r, Karl-J&uuml;rgen, Feliberto de la Cruz, Andy Schumann, Stefanie Koehler, Heinrich Sauer, Hugo Critchley, and Gerd Wagner. 2016. ?Functional 
Connectivity and Network Analysis of Midbrain and Brainstem Nuclei.? NeuroImage 134 (July):53?63.                                           
Diedrichsen, J&ouml;rn, Joshua H. Balsters, Jonathan Flavell, Emma Cussans, and Narender Ramnani. 2009. A Probabilistic MR Atlas of the Human 
Cerebellum. NeuroImage 46 (1): 39?46.                                                                                                    
Fairhurst, Merle, Katja Wiech, Paul Dunckley, and Irene Tracey. 2007. ?Anticipatory Brainstem Activity Predicts Neural Processing of Pain 
in Humans.? Pain 128 (1-2):101?10.                                                                                                        
Fan 2016 Cerebral Cortex; doi:10.1093/cercor/bhw157
Glasser, Matthew F., Timothy S. Coalson, Emma C. Robinson, Carl D. Hacker, John Harwell, Essa Yacoub, Kamil Ugurbil, et al. 2016. A 
Multi-Modal Parcellation of Human Cerebral Cortex. Nature 536 (7615): 171?78.                                                       
Keren, Noam I., Carl T. Lozar, Kelly C. Harris, Paul S. Morgan, and Mark A. Eckert. 2009. &#8220;In Vivo Mapping of the Human Locus Coeruleus.&#8221; 
NeuroImage 47 (4): 1261&#8211;67.                                                                                                               
Keuken, M. C., P-L Bazin, L. Crown, J. Hootsmans, A. Laufer, C. M&uuml;ller-Axt, R. Sier, et al. 2014. &#8220;Quantifying Inter-Individual Anatomical 
Variability in the Subcortex Using 7 T Structural MRI.&#8221; NeuroImage 94 (July): 40&#8211;46.                                                       
Krauth A, Blanc R, Poveda A, Jeanmonod D, Morel A, Sz&eacute;kely G. (2010) A mean three-dimensional atlas of the human thalamus: generation from 
multiple histological data. Neuroimage. 2010 Feb 1;49(3):2053-62. Jakab A, Blanc R, Ber&eacute;nyi EL, Sz&eacute;kely G. (2012) Generation of            
Individualized Thalamus Target Maps by Using Statistical Shape Models and Thalamocortical Tractography. AJNR Am J Neuroradiol. 33:         
2110-2116, doi: 10.3174/ajnr.A3140                                                                                                         
Nash, Paul G., Vaughan G. Macefield, Iven J. Klineberg, Greg M. Murray, and Luke A. Henderson. 2009. ?Differential Activation of the Human 
Trigeminal Nuclear Complex by Noxious and Non-Noxious Orofacial Stimulation.? Human Brain Mapping 30 (11):3772?82.                         
Pauli 2018 Bioarxiv: CIT168 from Human Connectome Project data
Pauli, Wolfgang M., Amanda N. Nili, and J. Michael Tyszka. 2018. ?A High-Resolution Probabilistic in Vivo Atlas of Human Subcortical Brain 
Nuclei.? Scientific Data 5 (April): 180063.                                                                                                
Pauli, Wolfgang M., Randall C. O?Reilly, Tal Yarkoni, and Tor D. Wager. 2016. ?Regional Specialization within the Human Striatum for 
Diverse Psychological Functions.? Proceedings of the National Academy of Sciences of the United States of America 113 (7): 1907?12.  
Sclocco, Roberta, Florian Beissner, Gaelle Desbordes, Jonathan R. Polimeni, Lawrence L. Wald, Norman W. Kettner, Jieun Kim, et al. 2016. 
?Neuroimaging Brainstem Circuitry Supporting Cardiovagal Response to Pain: A Combined Heart Rate Variability/ultrahigh-Field (7 T)       
Functional Magnetic Resonance Imaging Study.? Philosophical Transactions. Series A, Mathematical, Physical, and Engineering Sciences 374 
(2067). rsta.royalsocietypublishing.org. https://doi.org/10.1098/rsta.2015.0189.                                                         
Shen, X., F. Tokoglu, X. Papademetris, and R. T. Constable. 2013. &#8220;Groupwise Whole-Brain Parcellation from Resting-State fMRI Data for 
Network Node Identification.&#8221; NeuroImage 82 (November): 403&#8211;15.                                                                        
Zambreanu, L., R. G. Wise, J. C. W. Brooks, G. D. Iannetti, and I. Tracey. 2005. ?A Role for the Brainstem in Central Sensitisation in 
Humans. Evidence from Functional Magnetic Resonance Imaging.? Pain 114 (3):397?407.                                                    
____________________________________________________________________________________________________________________________________________
</pre><img vspace="5" hspace="5" src="canlab_help_5_regression_walkthrough_22.png" alt=""> <img vspace="5" hspace="5" src="canlab_help_5_regression_walkthrough_23.png" alt=""> <h2 id="15">Block 15: Multivariate prediction from unbiased ROI averages</h2><pre class="codeinput">datno16.Y = datno16.X;  <span class="comment">% .Y is the outcome to be explained</span>

<span class="comment">% 5-fold cross validated prediction, stratified on outcome</span>

[cverr, stats, optout] = predict(datno16, <span class="string">'algorithm_name'</span>, <span class="string">'cv_lassopcrmatlab'</span>, <span class="string">'nfolds'</span>, 5);

<span class="comment">% Though many areas show some significant effects, these are not strong</span>
<span class="comment">% enough to obtain a meaningful out-of-sample prediction of Success</span>
</pre><pre class="codeoutput">Cross-validated prediction with algorithm cv_lassopcrmatlab,   5 folds
Regression mode (Gaussian) 
Completed fit for all data in:   0 hours   0 min  0 secs 
Regression mode (Gaussian) Fold 1/5 done in:   0 hours   0 min  0 sec
Regression mode (Gaussian) Fold 2/5 done in:   0 hours   0 min  0 sec
Regression mode (Gaussian) Fold 3/5 done in:   0 hours   0 min  0 sec
Regression mode (Gaussian) Fold 4/5 done in:   0 hours   0 min  0 sec
Regression mode (Gaussian) Fold 5/5 done in:   0 hours   0 min  0 sec

Total Elapsed Time =   0 hours   0 min  1 sec
</pre><p class="footer"><br><a href="https://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2018a</a><br></p></div><!--
##### SOURCE BEGIN #####
%% Load sample data
% BLOCK 1
% REPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASH-

% Load sample data using load_image_set(), which produces an fmri_data
% object. Data loading exceeds the scope of this tutorial, but a more
% indepth demosntration may be provided by canlab_help_2_load_a_sample_dataset.m

% These are [Reappraise - Look Neg] contrast images, one image per person

[image_obj, networknames, imagenames] = load_image_set('emotionreg');

% Summarize and print a table with characteristics of the data:
desc = descriptives(image_obj);

% Load behavioral data
% This is "Reappraisal success", one score per person, in our example
% If you do not have the file on your path, you will get an error.

beh = importdata('Wager_2008_emotionreg_behavioral_data.txt')
success = beh.data(:, 2);           % Reappraisal success

% Load a mask that we would like to apply to analysis/results

mask = which('gray_matter_mask.img')

maskdat = fmri_data(mask, 'noverbose');

%% Visualize the mask
% REPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASH-
% BLOCK 2: Check mask

% This is an underlay brain:

o2 = canlab_results_fmridisplay([], 'noverbose');
drawnow, snapnow;

% This is a basic gray-matter mask we will use for analysis:
% It can help reduce multiple comparisons relative to whole-image analysis
% but we should still look at what's happening in ventricles and
% out-of-brain space to check for artifacts.

o2 = addblobs(o2, region(maskdat));

% Add a title to montage 1, the axial slice montage:
o2 = title_montage(o2, 1, 'Mask image: Gray matter with border');

drawnow, snapnow;

%% Visualize summary of brain coverage
% REPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASH-
% BLOCK 3: Check that we have valid data in most voxels for most subjects

% Show summary of coverage - how many images have non-zero, non-NaN values in each voxel

o2 = removeblobs(o2);
o2 = montage(region(desc.coverage_obj), o2, 'trans', 'transvalue', .3);

o2 = title_montage(o2, 1, 'Coverage in images');

% OR:
% orthviews(desc.coverage_obj, 'continuous');

%% BLOCK 4
% REPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASH-
% Check histograms of individual subjects for global shifts in contrast values

% The 'histogram' object method will allow you to examine a series of
% images in one panel each.  See help fmri_data.histogram for more options,
% including breakdown by tissue type.

hist_han = histogram(image_obj, 'byimage', 'color', 'b');

% This shows us that some of the images do not have the same mean as the
% others. This is fairly common, as individual subjects can often have
% global artifacts (e.g., task-correlated head motion or outliers) that
% influence the whole contrast image, even when baseline conditions are
% supposed to be "subtracted out".  
%
% It suggests that we may want to do an outlier analysis and/or standardize 
% the scale of the images. We'll return to this below.

%% BLOCK 5
% REPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASH-
% Examine predictor distribution and leverages
% Leverage is a measure of how much each point influences the regression
% line. The more extreme the predictor value, the higher the leverage.
% Outliers will have very high leverage. High-leverage behavioral observations 
% can strongly influence, and sometimes invalidate, an analysis.

X = scale(success, 1); X(:, end+1) = 1;         % A simple design matrix, behavioral predictor + intercept
H = X * inv(X'* X) * X';                        % The "Hat Matrix", which produces fits. Diagonals are leverage

create_figure('levs', 2, 1); 
plot(success, 'o', 'MarkerFaceColor', [0 .3 .7], 'LineWidth', 3); 
set(gca, 'FontSize', 24); 
xlabel('Subject number'); 
ylabel('Reappraisal success');

subplot(2, 1, 2);
plot(diag(H), 'o', 'MarkerFaceColor', [0 .3 .7], 'LineWidth', 3); 
set(gca, 'FontSize', 24); 
xlabel('Subject number'); 
ylabel('Leverage');


%% BLOCK 6
% Run regression
% The regress method takes predictors that are attached in the object's X
% attribute (X stands for design matrix) and regresses each voxel's
% activity (y) on the set of regressors.  
% This is a group analysis, in this case correlating brain activity with
% reappraisal success at each voxel.

% .X must have the same number of observations, n, in an n x k matrix.
% n images is the number of COLUMNS in image_obj.dat

% mean-center success scores and attach them to image_obj in image_obj.X
image_obj.X = scale(success, 1);

% runs the regression at each voxel and returns statistic info and creates
% a visual image.  regress = multiple regression.

out = regress(image_obj);

% out has statistic_image objects that have information about the betas
% (slopes) b, t-values and p-values (t), degrees of freedom (df), and sigma
% (error variance).  The critical one is out.t.
% out = 
% 
%         b: [1x1 statistic_image]
%         t: [1x1 statistic_image]
%        df: [1x1 fmri_data]
%     sigma: [1x1 fmri_data]

% Now let's try thresholding the image at q < .05 fdr-corrected.
 t = threshold(out.t, .05, 'fdr');
 
% ...and display

o2 = montage(t, 'trans');
o2 = title_montage(o2, 2, 'Behavioral predictor: Reappraisal success');
o2 = title_montage(o2, 4, 'Intercept: Group average activation');
snapnow

% Or:
% orthviews(t)

% This is a multiple regression, and there are two output t images, one for
% each regressor.  We've only entered one regressor, why two images?  The program always
% adds an intercept by default.  The intercept is always the last column of the design matrix

% Image   1  <REPLACE_WITH_DASH_DASH- brain contrast values correlated with "success"
% Positive effect:   0 voxels, min p-value: 0.00001192
% Negative effect:   0 voxels, min p-value: 0.00170529
% Image   2 FDR q < 0.050 threshold is 0.003193
% 
% Image   2 <REPLACE_WITH_DASH_DASH- intercept, because we have mean-centered, this is the
% average group effect (when success = average success).  "Reapp - Look"
% contrast in the whole group.
% Positive effect: 3133 voxels, min p-value: 0.00000000
% Negative effect:  51 voxels, min p-value: 0.00024068

% re-threshold at p < .005 uncorrected
t = threshold(out.t, .005, 'unc');

o2 = montage(t, 'trans');
o2 = title_montage(o2, 2, 'Behavioral predictor: Reappraisal success');
o2 = title_montage(o2, 4, 'Intercept: Group average activation');
snapnow


%% BLOCK 7
% Display the results on slices

% multi_threshold lets us see the blobs with significant voxels at the
% highest (most stringent) threshold, and voxels that are touching
% (contiguous) down to the lowest threshold, in different colors.

o2 = multi_threshold(out.t, 'thresh', [.005 .01 .05], 'sizethresh', [5 1 1]);

o2 = title_montage(o2, 2, 'Behavioral predictor: Reappraisal success');
o2 = title_montage(o2, 4, 'Intercept: Group average activation');
snapnow

%% BLOCK 8: Look for signal in ventricles, white matter, outside of brain
%
% We want to diagnose potential problems due to outliers, etc...

% Strategy 1:  Apply a very liberal threshold.
t = threshold(out.t, .05, 'unc');

figure;
o2 = canlab_results_fmridisplay;
o2 = addblobs(o2, region(t));

o2 = title_montage(o2, 5, 'Liberal threshold: .05 uncorrected');


% Strategy 2: Extract mean signal from WM and ventricles
m = extract_gray_white_csf(image_obj);

barplot_columns(m, 'title', 'Gray white CSF per subject', 'names', {'Gray' 'White' 'CSF'}, 'XTick', [1:3], 'colors', {[1 0 0] [0 .7 0] [0 0 1]});

ylabel('Contrast values');


% global_gray_white_csf = extract_gray_white_csf(image_obj);
% corr([global_gray_white_csf diag(H) success])

%% BLOCK 9: Compare results to meta-analysis for positive controls

% Map from neurosynth.org 
metaimg = which('emotion regulation_pAgF_z_FDR_0.01_8_14_2015.nii')
r = region(metaimg);

o2 = removeblobs(o2);
o2 = addblobs(o2, r, 'maxcolor', [1 0 0], 'mincolor', [.7 .2 .5]);

o2 = title_montage(o2, 5, 'Neurosynth mask: Emotion regulation');

%% BLOCK 10: Apply gray-matter mask and show FDR-thresholded results

% This can increase power by focusing on areas we think there are plausible
% effects. First is the success effect (regressor) and second is the
% intercept (group average contrast) map.

%t = threshold(out.t, .05, 'fdr', 'mask', mask);

t = apply_mask(out.t, maskdat);
t = threshold(t, .05, 'fdr');

% Select the reappraisal success effect only and show it
t1 = select_one_image(t, 1);

o2 = removeblobs(o2);
o2 = addblobs(o2, region(t1));

o2 = title_montage(o2, 5, 'Behavioral predictor, gray-matter masked, q < .05 FDR');


%% BLOCK 11: Refine analysis by removing outlier and ranking predictor values

% exclude high-leverage subject 16
datno16 = image_obj;
datno16.dat(:, 16) = [];

% try rank: robust...
% Ranking is a kind of nonparametric

datno16.X = success;
datno16.X(16) = [];
datno16.X = scale(rankdata(datno16.X), 1);

% Re-run regression
out = regress(datno16);

% Apply gray matter mask and threshold
t = apply_mask(out.t, maskdat);
t = threshold(t, .005, 'unc');
%orthviews(t)

o2 = multi_threshold(t, 'thresh', [.005 .01 .05], 'sizethresh', [1 1 1]);

o2 = title_montage(o2, 2, 'Behavioral predictor: Reappraisal success');
o2 = title_montage(o2, 4, 'Intercept: Group average activation');
snapnow


% Select the reappraisal success effect only and write it to disk

t1 = select_one_image(t, 1);
t1.fullpath = fullfile(pwd, 'Reapp_Success_005_outlier_removed.nii');
write(t1)

%% Block 12: Extract and plot data from (biased) regions of interest
% Let's visualize the correlation scatterplots in the areas we've
% discovered as related to Success

% Select the Success regressor map
r = region(t1);

% Autolabel regions and print a table
r = table(r);

% Make a montage showing each significant region
montage(r, 'colormap', 'regioncenters');

%% Block 13: Extract and plot data from (biased) regions of interest
% Let's visualize the correlation scatterplots in the areas we've
% discovered as related to Success

% Extract data from all regions
% r(i).dat has the averages for each subject across voxels for region i
r = extract_data(r, datno16);

% Select only regions with 3+ voxels
wh = cat(1, r.numVox) < 3;
r(wh) = [];

% Set up the scatterplots
nrows = floor(sqrt(length(r)));
ncols = ceil(length(r) ./ nrows);
create_figure('scatterplot_region', nrows, ncols);

% Make a loop and plot each region
for i = 1:length(r)
    
    subplot(nrows, ncols, i);

    % Use this line for non-robust correlations:
    %plot_correlation_samefig(r(i).dat, datno16.X);
    
    % Use this line for robust correlations:
    plot_correlation_samefig(r(i).dat, datno16.X, [], 'k', 0, 1);
  
    set(gca, 'FontSize', 12);
    xlabel('Reappraise - Look Neg brain response');
    ylabel('Reappraisal success');
    
    % input('Press a key to continue');
    
end


%% Block 14: Extract and plot data from unbiased regions of interest
% Let's visualize the correlation scatterplots in some meta-analysis
% derived ROIs

% Select the Neurosynth meta-analysis map
r = region(metaimg);

% Extract data from all regions
r = extract_data(r, datno16);

% Select only regions with 20+ voxels
wh = cat(1, r.numVox) < 20;
r(wh) = [];

r = table(r);

% Make a montage showing each significant region
montage(r, 'colormap', 'regioncenters');

% Set up the scatterplots
nrows = floor(sqrt(length(r)));
ncols = ceil(length(r) ./ nrows);
create_figure('scatterplot_region', nrows, ncols);

% Make a loop and plot each region
for i = 1:length(r)
    
    subplot(nrows, ncols, i);

    % Use this line for non-robust correlations:
    %plot_correlation_samefig(r(i).dat, datno16.X);
    
    % Use this line for robust correlations:
    plot_correlation_samefig(r(i).dat, datno16.X, [], 'k', 0, 1);
  
    set(gca, 'FontSize', 12);
    xlabel('Brain');
    ylabel('Success');
    title(' ');
   
end

%% Block 15: Multivariate prediction from unbiased ROI averages


datno16.Y = datno16.X;  % .Y is the outcome to be explained

% 5-fold cross validated prediction, stratified on outcome

[cverr, stats, optout] = predict(datno16, 'algorithm_name', 'cv_lassopcrmatlab', 'nfolds', 5);

% Though many areas show some significant effects, these are not strong
% enough to obtain a meaningful out-of-sample prediction of Success


##### SOURCE END #####
--></body></html>