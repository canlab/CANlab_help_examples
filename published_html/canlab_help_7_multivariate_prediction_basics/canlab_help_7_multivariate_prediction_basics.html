
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>Multivariate prediction of a continuous outcome</title><meta name="generator" content="MATLAB 9.4"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2020-01-13"><meta name="DC.source" content="canlab_help_7_multivariate_prediction_basics.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:14px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.8em; color:#2C2D92; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.4em; color:#363538; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#363538; font-weight:bold; line-height:140%; }

a { color:#4B4BA8; text-decoration:none; }
a:hover { color:#2AAFDF; text-decoration:underline; }
a:visited { color:#4B4BA8; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:14px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h1>Multivariate prediction of a continuous outcome</h1><!--introduction--><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#2">About the pain dataset</a></li><li><a href="#3">Load dataset with images and print descriptives</a></li><li><a href="#4">Collect some variables we need</a></li><li><a href="#5">Plot the ratings</a></li><li><a href="#6">Prediction options and choices</a></li><li><a href="#7">Run the Base model</a></li><li><a href="#10">Plot cross-validated predicted vs. actual outcomes</a></li><li><a href="#11">Summarize within-person classification accuracy</a></li><li><a href="#12">Visualize the classifier weight map</a></li><li><a href="#13">Bootstrap weights: Get most reliable weights and p-values for voxels</a></li><li><a href="#14">Try normalizing/scaling data</a></li><li><a href="#15">Try selecting an a priori 'network of interest'</a></li><li><a href="#16">re-run prediction and plot</a></li><li><a href="#17">Other ideas</a></li></ul></div><p>This walkthrough uses the <tt>predict()</tt> method for fmri_data objects to predict a continuous outcome using cross-validated principal component regression (PCR / LASSO-PCR).</p><p>The <tt>predict()</tt> method can run multiple algorithms with a range of options. The main ones used in the CANlab are SVM for two-choice classification and PCR for regression.</p><h2 id="2">About the pain dataset</h2><p>-------------------------------------------------------------------------</p><p>The dataset contains data from 33 participants, with brain responses to six levels of heat (non-painful and painful). Each image is the average over several (4-8) trials of heat delivered at a single stimulus intensity, ranging from 44.3 - 49.3 degrees C in one-degree increments. Each image is also paired with an average reported pain value for that set of trials, rated immmediately after heat experience.</p><p>This dataset is interesting for mixed-effects and predictive analyses, as it has both within-person and between-person sources of variance.</p><p>Aspects of this data appear in these papers: Wager, T.D., Atlas, L.T., Lindquist, M.A., Roy, M., Choong-Wan, W., Kross, E. (2013). An fMRI-Based Neurologic Signature of Physical Pain. The New England Journal of Medicine. 368:1388-1397 (Study 2)</p><p>Woo, C. -W., Roy, M., Buhle, J. T. &amp; Wager, T. D. (2015). Distinct brain systems mediate the effects of nociceptive input and self-regulation on pain. PLOS Biology. 13(1): e1002036. doi:10.1371/journal.pbio.1002036</p><p>Lindquist, Martin A., Anjali Krishnan, Marina L&oacute;pez-Sol&agrave;, Marieke Jepma, Choong-Wan Woo, Leonie Koban, Mathieu Roy, et al. 2015. ?Group-Regularized Individual Prediction: Theory and Application to Pain.? NeuroImage. <a href="http://www.sciencedirect.com/science/article/pii/S1053811915009982">http://www.sciencedirect.com/science/article/pii/S1053811915009982</a>.</p><h2 id="3">Load dataset with images and print descriptives</h2><p>This dataset is shared on figshare.com, under this link: https://figshare.com/s/ca23e5974a310c44ca93</p><p>Here is a direct link to the dataset file with the fmri_data object: https://ndownloader.figshare.com/files/12708989</p><p>The key variable is image_obj This is an fmri_data object from the CANlab Core Tools repository for neuroimaging data analysis. See https://canlab.github.io/</p><p>image_obj.dat contains brain data for each image (average across trials) image_obj.Y contains pain ratings (one average rating per image)</p><p>image_obj.additional_info.subject_id contains integers coding for which Load the data file, downloading from figshare if needed</p><pre class="codeinput">fmri_data_file = which(<span class="string">'bmrk3_6levels_pain_dataset.mat'</span>);

<span class="keyword">if</span> isempty(fmri_data_file)

    <span class="comment">% attempt to download</span>
    disp(<span class="string">'Did not find data locally...downloading data file from figshare.com'</span>)

    fmri_data_file = websave(<span class="string">'bmrk3_6levels_pain_dataset.mat'</span>, <span class="string">'https://ndownloader.figshare.com/files/12708989'</span>);

<span class="keyword">end</span>

load(fmri_data_file);

descriptives(image_obj);
</pre><pre class="codeoutput"> 
Source: BMRK3 dataset from CANlab, PI Tor Wager
Data: .dat contains 6 images per participant, activation estimates during heat on L arm from level 1(44.3 degrees C) to level 6(49.3), in 1 degree increments.
 

____________________________________________________________________________________________________________________________________________
 Wager, T.D., Atlas, L.T., Lindquist, M.A., Roy, M., Choong-Wan, W., Kross, E. (2013). An fMRI-Based Neurologic Signature of Physical Pain. 
The New England Journal of Medicine. 368:1388-1397 (Study 2)                                                                                
                                                                                                                                            
 Woo, C. -W., Roy, M., Buhle, J. T. &amp; Wager, T. D. (2015). Distinct brain systems mediate the effects of nociceptive input and 
self-regulation on pain. PLOS Biology. 13(1): e1002036. doi:10.1371/journal.pbio.1002036                                       
                                                                                                                               
Lindquist, Martin A., Anjali Krishnan, Marina L&oacute;pez-Sol&agrave;, Marieke Jepma, Choong-Wan Woo, Leonie Koban, Mathieu Roy, et al. 2015. 
&#8220;Group-Regularized Individual Prediction: Theory and Application to Pain.&#8221; NeuroImage.                                           
http://www.sciencedirect.com/science/article/pii/S1053811915009982.                                                              
                                                                                                                                 
____________________________________________________________________________________________________________________________________________
 
Summary of dataset
______________________________________________________
Images: 198	Nonempty: 198	Complete: 198
Voxels: 223707	Nonempty: 223707	Complete: 223707
Unique data values: 35583281
 
Min: -11.529	Max: 7.862	Mean: -0.004	Std: 0.227
 
    Percentiles      Values  
    ___________    __________

        0.1            -1.529
        0.5          -0.88055
          1          -0.67632
          5          -0.31891
         25         -0.087519
         50        0.00034864
         75          0.087461
         95           0.29757
         99           0.60449
       99.5           0.77676
       99.9            1.3177

 
</pre><h2 id="4">Collect some variables we need</h2><pre class="codeinput"><span class="comment">% subject_id is useful for cross-validation</span>
<span class="comment">%</span>
<span class="comment">% this used as a custom holdout set in fmri_data.predict() below will</span>
<span class="comment">% implement leave-one-subject-out cross-validation</span>

subject_id = image_obj.additional_info.subject_id;

<span class="comment">% ratings: reconstruct a subjects x temperatures matrix</span>
<span class="comment">% so we can plot it</span>
<span class="comment">%</span>
<span class="comment">% The command below does this only because we have exactly 6 conditions</span>
<span class="comment">% nested within 33 subjects and no missing data.</span>

ratings = reshape(image_obj.Y, 6, 33)';

<span class="comment">% temperatures are 44 - 49 (actually 44.3 - 49.3) in order for each person.</span>

temperatures = image_obj.additional_info.temperatures;
</pre><h2 id="5">Plot the ratings</h2><p>-------------------------------------------------------------------------</p><pre class="codeinput">create_figure(<span class="string">'ratings'</span>);
hold <span class="string">on</span>; plot(ratings', <span class="string">'-'</span>, <span class="string">'Color'</span>, [.7 .7 .7], <span class="string">'LineWidth'</span>, .5);
lineplot_columns(ratings, <span class="string">'color'</span>, [.7 .3 .3], <span class="string">'markerfacecolor'</span>, [1 .5 0]);
xlabel(<span class="string">'Temperature'</span>);
ylabel(<span class="string">'Rating'</span>);
set(gca, <span class="string">'XTickLabel'</span>, [44.3:49.3], <span class="string">'FontSize'</span>, 18);
</pre><img vspace="5" hspace="5" src="canlab_help_7_multivariate_prediction_basics_01.png" alt=""> <h2 id="6">Prediction options and choices</h2><p>There are many choices for algorithm and process/parameter tuning Here, we describe a constrained set of some of the most common choices</p><p>Base model: Linear, whole brain prediction</p><p>Outcome distribution: Continuous: Regression (LASSO-PCR) Categorical/2 choice: SVM, logistic regression</p><p>Model Options: - Feature selection (part of the brain) - Feature integration (new features)</p><p>Testing options: - Input data (what level) - Cross-validation - What testing metric (classification accuracy? RMSE?) and at what level (single-trial, condition averages, one-map-per-subject)</p><p>Inference options: - Component maps, voxel-wise maps - Thresholding (Bootstrapping, Permutation) - Global null hypothesis testing / sanity checks (Permutation)</p><h2 id="7">Run the Base model</h2><p><b>relevant functions:</b> predict method (fmri_data) predict_test_suite method (fmri_data)</p><p><b>Define the holdout set for cross-validation</b> We want to define custom holdout set.  If we use subject_id, which is a vector of integers with a unique integer per subject, then we are doing leave-one-subject-out cross-validation.</p><p>Let's build five-fold cross-validation set that leaves out ALL the images from a subject together. That way, we are always predicting out-of-sample (new individuals). If not, dependence across images from the same subjects may invalidate the estimate of predictive accuracy.</p><pre class="codeinput">holdout_set = zeros(size(subject_id));      <span class="comment">% holdout set membership for each image</span>
n_subjects = length(unique(subject_id));
C = cvpartition(n_subjects, <span class="string">'KFold'</span>, 5);
<span class="keyword">for</span> i = 1:5
    teidx = test(C, i);                     <span class="comment">% which subjects to leave out</span>
    imgidx = ismember(subject_id, find(teidx)); <span class="comment">% all images for these subjects</span>
    holdout_set(imgidx) = i;
<span class="keyword">end</span>
</pre><p><b>Run the prediction</b></p><pre class="codeinput">algoname = <span class="string">'cv_lassopcr'</span>; <span class="comment">% cross-validated penalized regression. Predict pain ratings</span>

[cverr, stats, optout] = predict(image_obj, <span class="string">'algorithm_name'</span>, algoname, <span class="string">'nfolds'</span>, holdout_set);
</pre><pre class="codeoutput">Cross-validated prediction with algorithm cv_lassopcr,   5 folds

Completed fit for all data in:   0 hours   0 min  5 secs 
Fold 1/5 done in:   0 hours   0 min  4 sec
Fold 2/5 done in:   0 hours   0 min  3 sec
Fold 3/5 done in:   0 hours   0 min  3 sec
Fold 4/5 done in:   0 hours   0 min  3 sec
Fold 5/5 done in:   0 hours   0 min  4 sec

Total Elapsed Time =   0 hours   0 min 23 sec
</pre><h2 id="10">Plot cross-validated predicted vs. actual outcomes</h2><p>Critical fields in stats output structure: stats.Y = actual outcomes stats.yfit = cross-val predicted outcomes pred_outcome_r: Correlation between .yfit and .Y weight_obj: Weight map used for prediction (across all subjects).</p><pre class="codeinput"><span class="comment">% Continuous outcomes:</span>

create_figure(<span class="string">'scatterplots'</span>, 1, 2);
plot(stats.yfit, stats.Y, <span class="string">'o'</span>)
axis <span class="string">tight</span>
refline
xlabel(<span class="string">'Predicted pain'</span>);
ylabel(<span class="string">'Observed pain'</span>);

<span class="comment">% Consider that each subject has their own series of multiple observations.</span>
<span class="comment">% Separate subjects into multiple cells, which will be plotted as separate</span>
<span class="comment">% individual lines:</span>

n = max(subject_id);

<span class="keyword">for</span> i = 1:n
    YY{i} = stats.Y(subject_id == i);
    Yfit{i} = stats.yfit(subject_id == i);
<span class="keyword">end</span>

subplot(1, 2, 2);
line_plot_multisubject(Yfit, YY, <span class="string">'nofigure'</span>);
xlabel(<span class="string">'Predicted pain'</span>);
ylabel(<span class="string">'Observed pain'</span>);
axis <span class="string">tight</span>

<span class="comment">% Yfit is a cell array, one cell per subject, with fitted values for that subject</span>
</pre><pre class="codeoutput">
____________________________________________________________________________________________________________________________________________
Input data:
X scaling: No centering or z-scoring
Y scaling: No centering or z-scoring
                
Transformations:
No data transformations before plot
                                                                   
Correlations:                                                      
r = 0.54 across all observations, based on untransformed input data
                                                               
Stats on slopes after transormation, subject is random effect: 
Mean b = 1.24, t( 32) = 6.93, p = 0.000000, num. missing:   0  
                                                               
Average within-person r = 0.83 +- 0.31 (std)
 
Between-person r (across subject means) = 0.11
____________________________________________________________________________________________________________________________________________
</pre><img vspace="5" hspace="5" src="canlab_help_7_multivariate_prediction_basics_02.png" alt=""> <h2 id="11">Summarize within-person classification accuracy</h2><p>Classification can turn a continuous outcome, such as the magnitude of activity in an integrated model/pattern, into a binary choice (Type A or B, Yes/No, Pain/No pain). Classification accuracy is an easy-to-interpret metric, which makes it appealing to report.</p><p>But though it seems easy to understand, it is perhaps tricker to interpret than we might think.</p><p>Classification accuracy is not one metric, but a family of them. The accuracy you obtain depends on what type of classification you're doing, and what the units of analysis are. For example, is your model based on a single person (individualized for that person), or based on other participants' data? Are you classifying images corresponding to a single trial, or an average across a group of trials? Do you have knowledge about which conditions/images belong to the same person, or not? Here, we will: (a) use a model based on other people's data (cross-validated across participants) (b) classify images summarizing a group of trials, and (c) assume we have 2 images belonging to the same person, and we classify which is which</p><p>On this last point, there are two basic senses of classification, which assume different levels of background knowledge: - In <i>single-interval classification</i>, we do not know which images belong to which participants,   and we are classifying a single image as Type A or B based on whether   the response in the model is above or below a threshold. - In <i>forced-choice</i> classification, we have two images that we know come   from the same person, and we know one is Type A and one is Type B...or,   equivalently, that one is "more A" than the other (e.g., more painful).   The classifier tries to pick out which is the best answer.</p><p>Forced-choice classification is an inherently easier problem, with higher resulting accuracy, because (a) each participant serves as their own control (many sources of noise are canceled out), and (b) you assume more background knowledge, conferring an advantage.</p><p>In this case, we want to summarize each subject with two images: One high pain, and one low pain. We'll use stimulus intensity as the variable to classify here, and ask if we can predict which intensity is highest given pairs of images one degree apart for each participant.</p><pre class="codeinput"><span class="comment">% If the stim intensity increases by 1 degree, how often do we correctly</span>
<span class="comment">% predict an increase?</span>
diffs = cellfun(@diff, Yfit, <span class="string">'UniformOutput'</span>, false); <span class="comment">% successive increases in predicted values with increases in stim intensity</span>

<span class="comment">% Subjects (rows) x comparisons (cols), each comparison 1 degree apart</span>
diffs = cat(2, diffs{:})';

acc_per_subject = sum(diffs &gt; 0, 2) ./ size(diffs, 2);
acc_per_comparison = (sum(diffs &gt; 0) ./ size(diffs, 1))';
cohens_d_per_comparison = (mean(diffs) ./ std(diffs))';

fprintf(<span class="string">'Mean acc is : %3.2f%% across all successive 1 degree increments\n'</span>, 100*mean(acc_per_subject));

<span class="comment">% Create and display a table:</span>

comparison = {<span class="string">'45-44'</span> <span class="string">'46-45'</span> <span class="string">'47-46'</span> <span class="string">'48-47'</span> <span class="string">'49-48'</span>}';
results_table = table(comparison, acc_per_comparison, cohens_d_per_comparison);
disp(results_table)

<span class="comment">% 45-44 degrees, 46-45, 47-46, etc.</span>
</pre><pre class="codeoutput">Mean acc is : 74.55% across all successive 1 degree increments
    comparison    acc_per_comparison    cohens_d_per_comparison
    __________    __________________    _______________________

     '45-44'           0.60606                 0.041851        
     '46-45'           0.72727                  0.63584        
     '47-46'           0.84848                  0.91783        
     '48-47'           0.87879                   1.1669        
     '49-48'           0.66667                  0.43536        

</pre><h2 id="12">Visualize the classifier weight map</h2><p>The weight map is stored in <tt>stats.weight_obj</tt>, as a statistic_image class object. If we have requested bootstrapping, the image will be thresholded (0.05 uncorrected by default). Otherwise, it will be unthresholded, and we'll see weights everywhere within the analysis mask.</p><pre class="codeinput">w = stats.weight_obj;  <span class="comment">% This is an image object that we can view, etc.</span>

orthviews(w)

create_figure(<span class="string">'montage'</span>);
o2 = montage(w);
o2 = title_montage(o2, 5, <span class="string">'Predictive model weights'</span>);
</pre><pre class="codeoutput">Grouping contiguous voxels:   3 regions

ans =

  1&times;1 cell array

    {1&times;3 region}

Setting up fmridisplay objects
This takes a lot of memory, and can hang if you have too little.
Grouping contiguous voxels:   3 regions
sagittal montage: 4350 voxels displayed, 219357 not displayed on these slices
sagittal montage: 4292 voxels displayed, 219415 not displayed on these slices
sagittal montage: 3990 voxels displayed, 219717 not displayed on these slices
axial montage: 32223 voxels displayed, 191484 not displayed on these slices
axial montage: 34538 voxels displayed, 189169 not displayed on these slices
</pre><img vspace="5" hspace="5" src="canlab_help_7_multivariate_prediction_basics_03.png" alt=""> <img vspace="5" hspace="5" src="canlab_help_7_multivariate_prediction_basics_04.png" alt=""> <h2 id="13">Bootstrap weights: Get most reliable weights and p-values for voxels</h2><p>Here is an exercise: Re-run the predictive model, adding a 'bootstrap' flag. For a final analysis, at least 5K bootstrap samples is a good idea. For now, try it with just 1,000 bootstrap samples (and be prepared to wait a bit). What does the resulting weight map look like?</p><p>(This is not run in the example; see <tt>help fmri_data.predict</tt> for how to add the bootstrap option.)</p><h2 id="14">Try normalizing/scaling data</h2><p>One of the biggest sources of noise can be whole-image (or large-spatial-scale shifts) in image values across participants. These can be apparent even in contrast or "subtraction" images where various sources of noise and artifacts are supposed to have been "subtracted out". This may result from different chance correlations between task regressors and physiological noise or artifacts/outliers across individuals.</p><p>Rescaling the data can remove whole-brain signal. This should be used with caution, as you're also removing signal from the images, and the resulting weight maps should be interpreted as effects <b>relative</b> to the mean signal across the image. i.e., a task may be associated with whole-brain increases, and <b>relative</b> decreases in a local area that appear only after the global increase has been accounted for.</p><p>Another reason to rescale images is to ask whether the <b>pattern</b> of activity across the brain (or within a local area) is predictive of task state, after removing the overall intensity of activation. This relates to pattern information.</p><p>Here, we'll rescale the images and ask if this improves our cross-validated predictions or not. If it does, the global signal we've removed is likely more noise than signal. If not, the global signal may contain information about pain.</p><pre class="codeinput"><span class="comment">% z-score every image, removing the mean and dividing by the std.</span>
<span class="comment">% Remove some scaling noise, but also some signal...</span>
 image_obj = rescale(image_obj, <span class="string">'zscoreimages'</span>);

<span class="comment">% re-run prediction and plot</span>

[cverr, stats, optout] = predict(image_obj, <span class="string">'algorithm_name'</span>, algoname, <span class="string">'nfolds'</span>, holdout_set);

create_figure(<span class="string">'scatterplots'</span>);

<span class="keyword">for</span> i = 1:n
    YY{i} = stats.Y(subject_id == i);
    Yfit{i} = stats.yfit(subject_id == i);
<span class="keyword">end</span>

line_plot_multisubject(Yfit, YY, <span class="string">'nofigure'</span>);
xlabel(<span class="string">'Predicted pain'</span>);
ylabel(<span class="string">'Observed pain'</span>);
axis <span class="string">tight</span>

fprintf(<span class="string">'Overall prediction-outcome correlation is %3.2f\n'</span>, stats.pred_outcome_r)

figure; o2 = montage(stats.weight_obj);
o2 = title_montage(o2, 5, <span class="string">'Predictive model weights'</span>);
</pre><pre class="codeoutput">Cross-validated prediction with algorithm cv_lassopcr,   5 folds

Completed fit for all data in:   0 hours   0 min  5 secs 
Fold 1/5 done in:   0 hours   0 min  4 sec
Fold 2/5 done in:   0 hours   0 min  4 sec
Fold 3/5 done in:   0 hours   0 min  4 sec
Fold 4/5 done in:   0 hours   0 min  3 sec
Fold 5/5 done in:   0 hours   0 min  4 sec

Total Elapsed Time =   0 hours   0 min 23 sec

____________________________________________________________________________________________________________________________________________
Input data:
X scaling: No centering or z-scoring
Y scaling: No centering or z-scoring
                
Transformations:
No data transformations before plot
                                                                   
Correlations:                                                      
r = 0.54 across all observations, based on untransformed input data
                                                               
Stats on slopes after transormation, subject is random effect: 
Mean b = 1.00, t( 32) = 8.73, p = 0.000000, num. missing:   0  
                                                               
Average within-person r = 0.73 +- 0.32 (std)
 
Between-person r (across subject means) = 0.03
____________________________________________________________________________________________________________________________________________
Overall prediction-outcome correlation is 0.54
Setting up fmridisplay objects
This takes a lot of memory, and can hang if you have too little.
Grouping contiguous voxels:   3 regions
sagittal montage: 4350 voxels displayed, 219357 not displayed on these slices
sagittal montage: 4292 voxels displayed, 219415 not displayed on these slices
sagittal montage: 3990 voxels displayed, 219717 not displayed on these slices
axial montage: 32223 voxels displayed, 191484 not displayed on these slices
axial montage: 34538 voxels displayed, 189169 not displayed on these slices
</pre><img vspace="5" hspace="5" src="canlab_help_7_multivariate_prediction_basics_05.png" alt=""> <img vspace="5" hspace="5" src="canlab_help_7_multivariate_prediction_basics_06.png" alt=""> <h2 id="15">Try selecting an a priori 'network of interest'</h2><pre class="codeinput"><span class="comment">% re-load</span>
load(<span class="string">'bmrk3_6levels_pain_dataset.mat'</span>, <span class="string">'image_obj'</span>)

<span class="comment">% load an a priori pain-related mask and apply it</span>

mask = fmri_data(which(<span class="string">'pain_2s_z_val_FDR_05.img.gz'</span>)); <span class="comment">% in Neuroimaging_Pattern_Masks repository</span>

image_obj = apply_mask(image_obj, mask);

<span class="comment">% show mean data with mask</span>
m = mean(image_obj);
figure; o2 = montage(m);
o2 = title_montage(o2, 5, <span class="string">'Mean image data, masked'</span>);
</pre><pre class="codeoutput">Using default mask: /Users/torwager/Documents/GitHub/CanlabCore/CanlabCore/canlab_canonical_brains/Canonical_brains_surfaces/brainmask.nii
loading mask. mapping volumes. 
checking that dimensions and voxel sizes of volumes are the same. 
Pre-allocating data array. Needed: 1409312 bytes
Loading image number:     1
Elapsed time is 0.023690 seconds.
Image names entered, but fullpath attribute is empty. Getting path info.
Setting up fmridisplay objects
This takes a lot of memory, and can hang if you have too little.
Grouping contiguous voxels:  27 regions
sagittal montage: 1088 voxels displayed, 47895 not displayed on these slices
sagittal montage: 1171 voxels displayed, 47812 not displayed on these slices
sagittal montage: 980 voxels displayed, 48003 not displayed on these slices
axial montage: 7496 voxels displayed, 41487 not displayed on these slices
axial montage: 7950 voxels displayed, 41033 not displayed on these slices
</pre><img vspace="5" hspace="5" src="canlab_help_7_multivariate_prediction_basics_07.png" alt=""> <h2 id="16">re-run prediction and plot</h2><pre class="codeinput">[cverr, stats, optout] = predict(image_obj, <span class="string">'algorithm_name'</span>, algoname, <span class="string">'nfolds'</span>, holdout_set);

create_figure(<span class="string">'scatterplots'</span>);

<span class="keyword">for</span> i = 1:n
    YY{i} = stats.Y(subject_id == i);
    Yfit{i} = stats.yfit(subject_id == i);
<span class="keyword">end</span>

line_plot_multisubject(Yfit, YY, <span class="string">'nofigure'</span>);
xlabel(<span class="string">'Predicted pain'</span>);
ylabel(<span class="string">'Observed pain'</span>);
axis <span class="string">tight</span>

fprintf(<span class="string">'Overall prediction-outcome correlation is %3.2f\n'</span>, stats.pred_outcome_r)

figure; o2 = montage(stats.weight_obj);
o2 = title_montage(o2, 5, <span class="string">'Predictive model weights'</span>);
</pre><pre class="codeoutput">Cross-validated prediction with algorithm cv_lassopcr,   5 folds

Completed fit for all data in:   0 hours   0 min  1 secs 
Fold 1/5 done in:   0 hours   0 min  1 sec
Fold 2/5 done in:   0 hours   0 min  1 sec
Fold 3/5 done in:   0 hours   0 min  1 sec
Fold 4/5 done in:   0 hours   0 min  1 sec
Fold 5/5 done in:   0 hours   0 min  1 sec

Total Elapsed Time =   0 hours   0 min  4 sec

____________________________________________________________________________________________________________________________________________
Input data:
X scaling: No centering or z-scoring
Y scaling: No centering or z-scoring
                
Transformations:
No data transformations before plot
                                                                   
Correlations:                                                      
r = 0.55 across all observations, based on untransformed input data
                                                               
Stats on slopes after transormation, subject is random effect: 
Mean b = 1.07, t( 32) = 10.41, p = 0.000000, num. missing:   0 
                                                               
Average within-person r = 0.82 +- 0.26 (std)
 
Between-person r (across subject means) = 0.17
____________________________________________________________________________________________________________________________________________
Overall prediction-outcome correlation is 0.55
Setting up fmridisplay objects
This takes a lot of memory, and can hang if you have too little.
Grouping contiguous voxels:  27 regions
sagittal montage: 1088 voxels displayed, 47895 not displayed on these slices
sagittal montage: 1171 voxels displayed, 47812 not displayed on these slices
sagittal montage: 980 voxels displayed, 48003 not displayed on these slices
axial montage: 7496 voxels displayed, 41487 not displayed on these slices
axial montage: 7950 voxels displayed, 41033 not displayed on these slices
</pre><img vspace="5" hspace="5" src="canlab_help_7_multivariate_prediction_basics_08.png" alt=""> <img vspace="5" hspace="5" src="canlab_help_7_multivariate_prediction_basics_09.png" alt=""> <h2 id="17">Other ideas</h2><p>There are many ways to build from this basic analysis.</p><pre class="codeinput"><span class="comment">% Data</span>
<span class="comment">% ------------</span>
<span class="comment">% Examine/clean outliers</span>
<span class="comment">% Transformations of data: Scaling/centering to remove global mean,</span>
<span class="comment">% ventricle mean</span>
<span class="comment">% Identify unaccounted sources of variation/subgroups (experimenter sex,</span>
<span class="comment">% etc.)</span>

<span class="comment">% Features</span>
<span class="comment">% ------------</span>
<span class="comment">% A priori regions / meta-analysis</span>
<span class="comment">% Feature selection in cross-val loop: Univariate, recursive feature elim</span>
</pre><p class="footer"><br><a href="https://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2018a</a><br></p></div><!--
##### SOURCE BEGIN #####
%% Multivariate prediction of a continuous outcome

%%
% This walkthrough uses the |predict()| method for fmri_data objects to
% predict a continuous outcome using cross-validated principal component
% regression (PCR / LASSO-PCR). 
%
% The |predict()| method can run multiple algorithms with a range of options.
% The main ones used in the CANlab are SVM for two-choice classification and PCR for
% regression.

%% About the pain dataset
% REPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASH-
%
% The dataset contains data from 33 participants, with brain responses to six levels
% of heat (non-painful and painful). Each image is the average over several
% (4-8) trials of heat delivered at a single stimulus intensity, ranging
% from 44.3 - 49.3 degrees C in one-degree increments. Each image is also
% paired with an average reported pain value for that set of trials, rated
% immmediately after heat experience. 
%
% This dataset is interesting for mixed-effects and predictive analyses, as
% it has both within-person and between-person sources of variance.  
% 
% Aspects of this data appear in these papers:
% Wager, T.D., Atlas, L.T., Lindquist, M.A., Roy, M., Choong-Wan, W., Kross, E. (2013). 
% An fMRI-Based Neurologic Signature of Physical Pain. The New England Journal of Medicine. 368:1388-1397
% (Study 2)
%
% Woo, C. -W., Roy, M., Buhle, J. T. & Wager, T. D. (2015). Distinct brain systems 
% mediate the effects of nociceptive input and self-regulation on pain. PLOS Biology. 13(1): 
% e1002036. doi:10.1371/journal.pbio.1002036
%
% Lindquist, Martin A., Anjali Krishnan, Marina López-Solà, Marieke Jepma, Choong-Wan Woo, 
% Leonie Koban, Mathieu Roy, et al. 2015. ?Group-Regularized Individual Prediction: 
% Theory and Application to Pain.? NeuroImage. 
% http://www.sciencedirect.com/science/article/pii/S1053811915009982.
%
%% Load dataset with images and print descriptives
% This dataset is shared on figshare.com, under this link:
% https://figshare.com/s/ca23e5974a310c44ca93
%
% Here is a direct link to the dataset file with the fmri_data object:
% https://ndownloader.figshare.com/files/12708989
%
% The key variable is image_obj
% This is an fmri_data object from the CANlab Core Tools repository for neuroimaging data analysis.
% See https://canlab.github.io/
%
% image_obj.dat contains brain data for each image (average across trials)
% image_obj.Y contains pain ratings (one average rating per image)
%
% image_obj.additional_info.subject_id contains integers coding for which
% Load the data file, downloading from figshare if needed

fmri_data_file = which('bmrk3_6levels_pain_dataset.mat');

if isempty(fmri_data_file)
    
    % attempt to download
    disp('Did not find data locally...downloading data file from figshare.com')
    
    fmri_data_file = websave('bmrk3_6levels_pain_dataset.mat', 'https://ndownloader.figshare.com/files/12708989');
    
end

load(fmri_data_file);

descriptives(image_obj);

%% Collect some variables we need

% subject_id is useful for cross-validation
%
% this used as a custom holdout set in fmri_data.predict() below will
% implement leave-one-subject-out cross-validation

subject_id = image_obj.additional_info.subject_id;

% ratings: reconstruct a subjects x temperatures matrix
% so we can plot it
%
% The command below does this only because we have exactly 6 conditions
% nested within 33 subjects and no missing data. 

ratings = reshape(image_obj.Y, 6, 33)';

% temperatures are 44 - 49 (actually 44.3 - 49.3) in order for each person.

temperatures = image_obj.additional_info.temperatures;

%% Plot the ratings
% REPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASH-  

create_figure('ratings');
hold on; plot(ratings', '-', 'Color', [.7 .7 .7], 'LineWidth', .5);
lineplot_columns(ratings, 'color', [.7 .3 .3], 'markerfacecolor', [1 .5 0]);
xlabel('Temperature');
ylabel('Rating');
set(gca, 'XTickLabel', [44.3:49.3], 'FontSize', 18);

%% Prediction options and choices
% There are many choices for algorithm and process/parameter tuning 
% Here, we describe a constrained set of some of the most common choices
%
% Base model: Linear, whole brain prediction
% 
% Outcome distribution:
% Continuous: Regression (LASSO-PCR) Categorical/2 choice: SVM, logistic regression
%
% Model Options:
% - Feature selection (part of the brain)
% - Feature integration (new features)
%
% Testing options:
% - Input data (what level)
% - Cross-validation
% - What testing metric (classification accuracy? RMSE?) and at what level
% (single-trial, condition averages, one-map-per-subject)
%
% Inference options:
% - Component maps, voxel-wise maps
% - Thresholding (Bootstrapping, Permutation)
% - Global null hypothesis testing / sanity checks (Permutation)

%% Run the Base model
%
% *relevant functions:*
% predict method (fmri_data)                            
% predict_test_suite method (fmri_data) 
%

%% 
% *Define the holdout set for cross-validation*
% We want to define custom holdout set.  If we use subject_id, which is a vector of
% integers with a unique integer per subject, then we are doing
% leave-one-subject-out cross-validation. 
%
% Let's build five-fold cross-validation set that leaves out ALL the images
% from a subject together. That way, we are always predicting out-of-sample
% (new individuals). If not, dependence across images from the same
% subjects may invalidate the estimate of predictive accuracy.

holdout_set = zeros(size(subject_id));      % holdout set membership for each image
n_subjects = length(unique(subject_id));
C = cvpartition(n_subjects, 'KFold', 5);
for i = 1:5
    teidx = test(C, i);                     % which subjects to leave out
    imgidx = ismember(subject_id, find(teidx)); % all images for these subjects
    holdout_set(imgidx) = i;
end

%% 
% *Run the prediction*

algoname = 'cv_lassopcr'; % cross-validated penalized regression. Predict pain ratings

[cverr, stats, optout] = predict(image_obj, 'algorithm_name', algoname, 'nfolds', holdout_set);

%% Plot cross-validated predicted vs. actual outcomes
%
% Critical fields in stats output structure:
% stats.Y = actual outcomes
% stats.yfit = cross-val predicted outcomes
% pred_outcome_r: Correlation between .yfit and .Y
% weight_obj: Weight map used for prediction (across all subjects).
                 
% Continuous outcomes:

create_figure('scatterplots', 1, 2);
plot(stats.yfit, stats.Y, 'o')
axis tight
refline
xlabel('Predicted pain');
ylabel('Observed pain');

% Consider that each subject has their own series of multiple observations.
% Separate subjects into multiple cells, which will be plotted as separate
% individual lines:

n = max(subject_id);

for i = 1:n
    YY{i} = stats.Y(subject_id == i);
    Yfit{i} = stats.yfit(subject_id == i);
end

subplot(1, 2, 2);
line_plot_multisubject(Yfit, YY, 'nofigure');
xlabel('Predicted pain');
ylabel('Observed pain');
axis tight

% Yfit is a cell array, one cell per subject, with fitted values for that subject

%% Summarize within-person classification accuracy
% 
% Classification can turn a continuous outcome, such as the magnitude of
% activity in an integrated model/pattern, into a binary choice (Type A or
% B, Yes/No, Pain/No pain). Classification accuracy is an easy-to-interpret
% metric, which makes it appealing to report. 
% 
% But though it seems easy to understand, it is perhaps tricker to
% interpret than we might think.
%
% Classification accuracy is not one metric, but a family of them. The accuracy
% you obtain depends on what type of classification you're doing, and what the
% units of analysis are. For example, is your model based on a single person 
% (individualized for that person), or based on other participants' data? 
% Are you classifying images corresponding to a single trial, or an average across a group of trials? 
% Do you have knowledge about which conditions/images belong to the same person, or not?
% Here, we will:
% (a) use a model based on other people's data (cross-validated across participants)
% (b) classify images summarizing a group of trials, and
% (c) assume we have 2 images belonging to the same person, and we classify
% which is which
%
% On this last point, there are two basic senses of classification, which assume different 
% levels of background knowledge:
% - In _single-interval classification_, we do not know which images belong to which participants,
%   and we are classifying a single image as Type A or B based on whether
%   the response in the model is above or below a threshold. 
% - In _forced-choice_ classification, we have two images that we know come
%   from the same person, and we know one is Type A and one is Type B...or,
%   equivalently, that one is "more A" than the other (e.g., more painful).
%   The classifier tries to pick out which is the best answer.
%
% Forced-choice classification is an inherently easier problem, with higher
% resulting accuracy, because (a) each participant serves as their own control
% (many sources of noise are canceled out), and (b) you assume more
% background knowledge, conferring an advantage. 
%
% In this case, we want to summarize each subject with two images: One high
% pain, and one low pain. We'll use stimulus intensity as the variable to
% classify here, and ask if we can predict which intensity is highest given
% pairs of images one degree apart for each participant.

% If the stim intensity increases by 1 degree, how often do we correctly
% predict an increase?
diffs = cellfun(@diff, Yfit, 'UniformOutput', false); % successive increases in predicted values with increases in stim intensity

% Subjects (rows) x comparisons (cols), each comparison 1 degree apart
diffs = cat(2, diffs{:})';

acc_per_subject = sum(diffs > 0, 2) ./ size(diffs, 2);
acc_per_comparison = (sum(diffs > 0) ./ size(diffs, 1))';
cohens_d_per_comparison = (mean(diffs) ./ std(diffs))';

fprintf('Mean acc is : %3.2f%% across all successive 1 degree increments\n', 100*mean(acc_per_subject));

% Create and display a table:

comparison = {'45-44' '46-45' '47-46' '48-47' '49-48'}';
results_table = table(comparison, acc_per_comparison, cohens_d_per_comparison);
disp(results_table)

% 45-44 degrees, 46-45, 47-46, etc.

%% Visualize the classifier weight map
%
% The weight map is stored in |stats.weight_obj|, as a statistic_image
% class object. If we have requested bootstrapping, the image will be
% thresholded (0.05 uncorrected by default). Otherwise, it will be unthresholded,
% and we'll see weights everywhere within the analysis mask.

w = stats.weight_obj;  % This is an image object that we can view, etc.

orthviews(w)

create_figure('montage');
o2 = montage(w);
o2 = title_montage(o2, 5, 'Predictive model weights');


%% Bootstrap weights: Get most reliable weights and p-values for voxels
%
% Here is an exercise: 
% Re-run the predictive model, adding a 'bootstrap' flag. 
% For a final analysis, at least 5K bootstrap samples is a good idea.
% For now, try it with just 1,000 bootstrap samples (and be prepared to
% wait a bit). What does the resulting weight map look like?
%
% (This is not run in the example; see |help fmri_data.predict| for how to
% add the bootstrap option.)

%% Try normalizing/scaling data
%
% One of the biggest sources of noise can be whole-image (or
% large-spatial-scale shifts) in image values across participants. These can
% be apparent even in contrast or "subtraction" images where various
% sources of noise and artifacts are supposed to have been "subtracted
% out". This may result from different chance correlations between task
% regressors and physiological noise or artifacts/outliers across
% individuals. 
%
% Rescaling the data can remove whole-brain signal. This should be used
% with caution, as you're also removing signal from the images, and the
% resulting weight maps should be interpreted as effects *relative* to the mean
% signal across the image. i.e., a task may be associated with whole-brain
% increases, and *relative* decreases in a local area that appear only
% after the global increase has been accounted for.
%
% Another reason to rescale images is to ask whether the *pattern* of
% activity across the brain (or within a local area) is predictive of task
% state, after removing the overall intensity of activation. This relates
% to pattern information.
%
% Here, we'll rescale the images and ask if this improves our
% cross-validated predictions or not. If it does, the global signal we've
% removed is likely more noise than signal. If not, the global signal may
% contain information about pain.

% z-score every image, removing the mean and dividing by the std.
% Remove some scaling noise, but also some signal...
 image_obj = rescale(image_obj, 'zscoreimages');

% re-run prediction and plot

[cverr, stats, optout] = predict(image_obj, 'algorithm_name', algoname, 'nfolds', holdout_set);

create_figure('scatterplots');

for i = 1:n
    YY{i} = stats.Y(subject_id == i);
    Yfit{i} = stats.yfit(subject_id == i);
end

line_plot_multisubject(Yfit, YY, 'nofigure');
xlabel('Predicted pain');
ylabel('Observed pain');
axis tight

fprintf('Overall prediction-outcome correlation is %3.2f\n', stats.pred_outcome_r)

figure; o2 = montage(stats.weight_obj);
o2 = title_montage(o2, 5, 'Predictive model weights');

%% Try selecting an a priori 'network of interest'

% re-load
load('bmrk3_6levels_pain_dataset.mat', 'image_obj')

% load an a priori pain-related mask and apply it

mask = fmri_data(which('pain_2s_z_val_FDR_05.img.gz')); % in Neuroimaging_Pattern_Masks repository

image_obj = apply_mask(image_obj, mask);

% show mean data with mask
m = mean(image_obj);
figure; o2 = montage(m);
o2 = title_montage(o2, 5, 'Mean image data, masked');


%% re-run prediction and plot

[cverr, stats, optout] = predict(image_obj, 'algorithm_name', algoname, 'nfolds', holdout_set);

create_figure('scatterplots');

for i = 1:n
    YY{i} = stats.Y(subject_id == i);
    Yfit{i} = stats.yfit(subject_id == i);
end

line_plot_multisubject(Yfit, YY, 'nofigure');
xlabel('Predicted pain');
ylabel('Observed pain');
axis tight

fprintf('Overall prediction-outcome correlation is %3.2f\n', stats.pred_outcome_r)

figure; o2 = montage(stats.weight_obj);
o2 = title_montage(o2, 5, 'Predictive model weights');

%% Other ideas
% There are many ways to build from this basic analysis.

% Data
% REPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASH
% Examine/clean outliers
% Transformations of data: Scaling/centering to remove global mean,
% ventricle mean
% Identify unaccounted sources of variation/subgroups (experimenter sex,
% etc.)

% Features
% REPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASH
% A priori regions / meta-analysis
% Feature selection in cross-val loop: Univariate, recursive feature elim


##### SOURCE END #####
--></body></html>